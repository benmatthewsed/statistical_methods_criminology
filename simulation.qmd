# Simulation

Sometimes when we fit a statistical model we just want to read off a coefficient in the model and that is our key result. However, sometimes we want to calculate some other quantity from our model and focus on that instead. For general applications of this approach see Gelman and Pardoe (2007) and King et al. (2000).

TO ADD: what's your estimand?


For example, [@hunterEquityJusticeCrime2016] use the results of a fitted regression model to calculate a measure they call 'Victimization Divide'. This measure is a way to describe how victimization inequality has changed over time - a very important thing for criminologists to care about.

This measure is defined as

(ratio of victimization rates in year 2 - 1) - (ratio of victimization rates in year 2 - 1) / (ratio of victimization rates in year one - 1)

in R code this might look like

```{r}

victim_divide <- function(base_y1, base_y2){
  ((base_y2 - 1) - (base_y1 - 1)) / (base_y1 - 1)
}

```

this is analogous to exploring the percentage change in victimization inequality between two comparison years.

However, if these ratios of rates come from a statistical model they will also come with some uncertainty measure, such as a standard error. In that case it's not quite so clear how to incorporate this uncertainty into the calculation of the VD. One way to do this is to use the regression model to simulate a bunch of coefficients values and then calculate the VD for each one.

Matthews and McVie (2024; see also Matthews 2024) demonstrate this process. The table below shows the prevalence of victimization (for all crime types) for men and women as reported in the Crime Survey for England and Wales for 2015 and 2020 (Office for National Statistics, n.d.). We can see that the prevalence of victimization was higher for men than women in both years, and that it increased for men (by 3.0 percentage points) and women (by 3.6 percentage points) between 2015 and 2020.

```{r}
# set seed and load packages
set.seed(nchar("vict divide") ^ 4)

library(MASS)
library(tidyverse)



dat <-
  tribble(
    ~prev, ~year, ~sex, ~n,
    0.167, "2015", "men", 15030,
    0.153, "2015", "women", 18320,
    0.197, "2020", "men", 15505,
    0.189, "2020", "women", 18230
  )
# calculate the number of victims
dat <-
  dat |> 
  mutate(vict = as.integer(n * prev))

```


We can fit a simple regression model to the data in each year to calculate the ratio of the odds of being a victim for men and women. Model 1 shows a statistically significant difference for men (compared to women) in 2015, with men having 11% greater odds of being a victim of crime. In contrast, Model 2 finds that men had a 5% greater odds of being a victim of crime than women in 2020; however, this difference does not meet the 95% threshold for statistical significance.

```{r}

model2020 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2020"))


model2015 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2015"))


results_2020 <-
broom::tidy(model2020) %>%
mutate(est = exp(estimate))

results_2015 <-
broom::tidy(model2015) %>%
mutate(est = exp(estimate))

```

Calculating the VD for these two results, based on the odds ratios from the two models, shows that victimization inequality decreased by 52% between the two years. Victimiztion inequality fell by more than half!

```{r}

main_est <- victim_divide(base_y1 = results_2015$est[[2]],
base_y2 = results_2020$est[[2]])

```

However, this does not factor the uncertainty in these odds ratios into our results. To investigate this we can simulate 1,000 sets of coefficients from the models' results. Using 10,000 simulations from the modelsâ€™ results, we get 95% intervals for the VD of -0.95 and 0.22. Whilst our most of our simulations do indicate that victimization inequality has declined, this would not be considered 'statistically significant' at the usual 95% threshold.

```{r}


n_sims <- 1e5

draws_2020 <-
  MASS::mvrnorm(
    n_sims,
    coef(model2020),
    vcov(model2020)
  )

draws_2015 <-
  MASS::mvrnorm(
    n_sims,
    coef(model2015),
    vcov(model2015)
  )

draws_2015 <-
  draws_2015 %>%
  as.data.frame() %>%
  as_tibble() %>%
  mutate(est = exp(`fct_rev(sex)men`))

draws_2020 <-
  draws_2020 %>%
  as.data.frame() %>%
  as_tibble() %>%
  mutate(est = exp(`fct_rev(sex)men`))

# combine the results
sim_dat <-
  tibble(
    vd = victim_divide(base_y1 = draws_2015$est,
                       base_y2 = draws_2020$est)
  )

sim_dat %>%
  reframe(vds = quantile(vd, c(0.025, 0.975),
          vals = c(0.025, 0.975)))

```

The beauty of this simulation approach described by King et al (2000) is that it generalizes to any DQI, such as VD, and to any regression model specification. Say that instead of the VD were interested in the absolute difference in predicted victimization after controlling for other factors (like a marginal effect). Or maybe we have fitted a count model and we want to know the number of people reporting 2 or more victimization incidents - we can calculate this from our simulations whilst incorporating (some) uncertainty into our estimates.

## Limits to simulation approaches

But remember that our model is wrong!! (Greenland) This approach incorporates the uncertainty *as expressed by your model's standard errors*. This still makes a load of assumptions - basically that we've fit the right model and that we had the right data. In more complex cases - like most real-world research - we might be fitting a much more complicated statistical model with non-linear effects and interaction terms and all sorts. As we'll discuss in the next section, we might also want to cast a critical eye over the data that we are analysing before drawing our conclusions.
