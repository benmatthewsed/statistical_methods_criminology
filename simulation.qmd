# The data don't speak for themselves, part two: Presenting results

\_\_There's nothing special about simulation here we can also do this with delta method or whatever? I should say this?\_\_s

In Part One we talked about reasons we might be skeptical about our model coefficients, and looked at some methods which adjust coefficients to account for possible biases in the data. In this part we are also moving beyond a narrow focus on regression coefficients, but this time we are going to translate coefficients into more interesting and informative quantities. This is a great way to make results more informative and accessible [@kingMakingMostStatistical2000].

In simple statistical models it is possible to read off a coefficient directly as the quenatities that we are interested in. In more complex models, such as generalized linear models, we often want to convert the coefficients to express results in a more accessible way. For example, with logistic regression it is common to convert log-odds to odds ratios when interperting results. In poisson regression the model coefficients are also commonly expressed as rate ratios. 

These are valuable techniques, but do not scale well to more complicated models, where individual coefficients need to be interpreted in the context of other parts of the model  (McElreath). Fortunately there are techniques we can use to re-express model estimates in more meaningful quantities. A primary challenge in converting combinations of regression coefficients into more meaningful values is how to also express the uncertainty in these coefficients. In this chapter we will focus on a simulation approach to this task which can be applied to any generalized linear model.




### Quantities of interest


#### A complicated example: victimization divides

For example, [@hunterEquityJusticeCrime2016] use the results of a fitted regression model to calculate a measure they call 'Victimization Divide'. This measure is a way to describe how victimization inequality has changed over time - an important thing for criminologists to care about.

This measure is defined as

(ratio of victimization rates in year 2 - 1) - (ratio of victimization rates in year 2 - 1) / (ratio of victimization rates in year one - 1)

in R code this might look like

```{r}

victim_divide <- function(base_y1, base_y2){
  ((base_y2 - 1) - (base_y1 - 1)) / (base_y1 - 1)
}

```

this is analogous to exploring the percentage change in victimization inequality between two comparison years.

To calculate the ratio of victimization rates in years 1 and 2, Hunter and Tseloni fit a regression model (specifically a negative binomial model) to predict the number of burglary victimization incidents experienced by households in England and Wales in 1993 compared to 20089. They use the coefficients from these models as inputs into the Victimization Divide formula. Based on this analysis they conclude that burglary victimization inequality had increased for:

- single adult households compared to other households
- social renters compared to owner occupiers
- households without a car compared to those with one car
- households leaving their home unoccupied any amount of time on a typical weekday compared to those never leaving the home
- households in areas without neighbourhood watch compared to those with the scheme
- households earning at least £50,000 per annum compared to those on a £10,000–£29,999 annual income
- inner city residents compared to households in rural areas

However, Hunter and Tseloni's data come from the Crime Survey for England and Wales. As such we should also be interested in the uncertainty in these estimates as well as the point estimates themselves - as we discussed in session one, these data are a sample from the whole population of England and Wales, and there is uncertainty in how the data in our spreadsheet reflects the experiences of the whole population. However, in this case it's not quite so clear how to incorporate this uncertainty into the calculation of the VD. The simulation process described by King et al. (2000) and implemented in the R package clarify (https://github.com/iqss/clarify) provides one solution.

In simple terms this technique uses the regression model to simulate a set of coefficient values from the regression model's variance-covariance matrix and and then calculate the VD for each one these simulated coefficient values. Together these draws from the variance-covariance matrix gives a set of draws which reflect the uncertainty in each of the regression coefficients (as expressed in their standard errors) and the correlation between these coefficients (as expressed in the covariance between the coefficients). This can help us assess the uncertainty in points estimates of change in the VD, and assess whether any changes are 'statistically significant'.

Matthews and McVie (2024; see also Matthews 2024) demonstrate this process. The table below shows the prevalence of victimization (for all crime types) for men and women as reported in the Crime Survey for England and Wales for 2015 and 2020 (Office for National Statistics, n.d.). We can see that the prevalence of victimization was higher for men than women in both years, and that it increased for men (by 3.0 percentage points) and women (by 3.6 percentage points) between 2015 and 2020.

```{r}
# load packages

library(MASS)
library(tidyverse)



dat <-
  tribble(
    ~prev, ~year, ~sex, ~n,
    0.167, "2015", "men", 15030,
    0.153, "2015", "women", 18320,
    0.197, "2020", "men", 15505,
    0.189, "2020", "women", 18230
  )
# calculate the number of victims
dat <-
  dat |> 
  mutate(vict = as.integer(n * prev))

```

We can fit a simple regression model to the data in each year to calculate the log-odds of being a victim for men and women. In this example I fit a separate model for 2015 and 2020. Model 1 shows a statistically significant difference for men (compared to women) in 2015, with men having 11% greater odds of being a victim of crime. In contrast, Model 2 finds that men had a 5% greater odds of being a victim of crime than women in 2020 - however this difference does not meet the 95% threshold for statistical significance.

```{r}

model2020 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2020"))


model2015 <- 
  glm(cbind(vict, n - vict) ~ fct_rev(sex),
family = "binomial",
data = filter(dat, year == "2015"))


results_2020 <-
broom::tidy(model2020) %>%
mutate(est = exp(estimate))

results_2015 <-
broom::tidy(model2015) %>%
mutate(est = exp(estimate))

```

Calculating the VD for these two results, based on the odds ratios from the two models, shows that victimization inequality decreased by 52% between the two years. Victimiztion inequality fell by more than half!

```{r}

main_est <- victim_divide(base_y1 = results_2015$est[[2]],
base_y2 = results_2020$est[[2]])

```

However, this estimate of the VD is based only on the point estimates of the coefficients from the two models, and so it does not factor the uncertainty in these odds ratios into our results. As such we don't have a good sense of the uncertainty in this description of how victimization inequality as changed. To investigate this we can simulate 10,000 sets of coefficients from the two models' results and then calculate the VD for each one. 

The code below runs this simulation. 

#### Interlude: simulating random variables in R


```{r}

var1 <- 
rnorm(
  n = 1000,
  mean = 0,
  sd = 1
)

var2 <- 
rnorm(
  n = 1000,
  mean = 1,
  sd = 1
)


tibble(
  var1 = var1,
  var2 = var2
) |> 
  ggplot(aes(x = var1, y = var2)) +
  geom_point()


```

We can see that these variables are uncorrelated. If we use `mvrnorm` the resulting draws can be correlated.


```{r}

MASS::mvrnorm(
  n = 1000,
  mu = c(0, 0), # mu instead of mean
  Sigma = matrix(c(1, 0.9, 0.9, 1), nrow = 2, ncol = 2) # Sigma instead of sd
) |> 
  as.data.frame() |> 
  ggplot(aes(x = V1, y = V2)) +
  geom_point()
  


```

### Back to our simulation

It uses the `mvrnorm` function from the `{MASS}` library to simulate from a multivariate normal distribution, with the means of this distribution being the model coefficient point estimates, and the model's variance-covariance matrix describing the covariance between the simulated draws.

```{r}

# set seed
set.seed(nchar("vict divide") ^ 4)

n_sims <- 1e5

draws_2020 <-
  MASS::mvrnorm(
    n_sims,
    coef(model2020),
    vcov(model2020)
  )

draws_2015 <-
  MASS::mvrnorm(
    n_sims,
    coef(model2015),
    vcov(model2015)
  )

draws_2015 <-
  draws_2015 %>%
  as.data.frame() %>%
  as_tibble() %>%
  mutate(est = exp(`fct_rev(sex)men`))

draws_2020 <-
  draws_2020 %>%
  as.data.frame() %>%
  as_tibble() %>%
  mutate(est = exp(`fct_rev(sex)men`))

# combine the results
sim_dat <-
  tibble(
    vd = victim_divide(base_y1 = draws_2015$est,
                       base_y2 = draws_2020$est)
  )

sim_dat %>%
  reframe(vds = quantile(vd, c(0.025, 0.975),
          vals = c(0.025, 0.975)))

```

Using 10,000 simulations from the models’ results, we get 95% intervals for the VD of -0.95 and 0.22. Whilst our most of our simulations do indicate that victimization inequality has declined, this would not be considered 'statistically significant' at the usual 95% threshold.

The beauty of this simulation approach described by King et al (2000) is that it generalizes to any DQI, such as VD, and to any regression model specification. Say that instead of the VD were interested in the absolute difference in predicted victimization after controlling for other factors (like a marginal effect). Or maybe we have fitted a count model and we want to know the number of people reporting 2 or more victimization incidents - we can calculate this from our simulations whilst incorporating (some) uncertainty into our estimates.

Matthews and McVie (forthcoming) illustrate that there are many (many!) ways to empirically describe victimization inequality. Using these simulation methods means you can use a statistical model to describe any of these measures, and factor in the uncertainty implied by the model. This means that you have more scope to map your theoretical estimand onto your empirical results.


## Using the clarify package

In the example above we performed the simulation from each models' variance-covariance matrix ourselves. Whilst it's good to know how this works, in practice there are R packages which can do this for us. One good option is [clarify](https://cran.r-project.org/web/packages/clarify/index.html).

Here we reproduce our results using clarify. Instead of fitting a model to each year as above, clarify expects us to give it a single model object, so we re-express the two separate models (one for each year) as a single model, now including year as an interaction term with sex. The code below is a bit involved, but 

```{r}
# using clarify -----------------------------------------------------------

library(clarify)


mod1 <- glm(cbind(vict, n - vict) ~ fct_rev(sex) * year,
                 family = "binomial",
                 data = dat)

s <- sim(mod1,
         n = n_sims)


sim_victim_divide <- function(coefs) {
  men_2015 <- unname(coefs["fct_rev(sex)men"])
  men_interact <- unname(coefs["fct_rev(sex)men:year2020"])
  
  men_2020 <- men_2015 + men_interact
  
  or_2015 <- exp(men_2015)
  or_2020 <- exp(men_2020)
  
  victim_divide(or_2015, or_2020)
  
}


est2 <- sim_apply(s, 
                  FUN = sim_victim_divide)

tibble(x = as.vector(est2)) |> 
  reframe(vds = quantile(x, c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99)),
          vals = c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99))

```

Here we can see that the results are basically the same as our own simulation. Importantly there is inherent uncertainty in simulation results, so it's crucial to set the seed for the random number generator so you get the same results every time. You also need to run enough simulation draws to get a good estimate of the uncertainty. The default for clarify is 1000 draws. More simulations are always better, but will take longer - so there is a pragmatic aspect to how much time and computer resource you have to run simulations. 

We have conducted an initial simulation ourselves to understand how the process works, but in general I would recommend using a package like clarify if you conduct this kind of simulation. It's likely that professionally developed and maintained software will be less error-prone and more efficient than writing your own!



#### A simple example: difference in probabilities?

In the example above we focused on calculating a new quantity of interest from the coefficients of a fitted regression model.

This drew on a specific (possibly quite niche) criminological example. This example is closely related to the general approach of calculating 'marginal effects'. These techniques use the fact that with any general linear model we can get a predicted value of our outcome by plugging the coefficients into the regression equation to express effects of independent variables on the scale of the outcome. this could be a difference in the probability of being a victim of crime between men and women, or the difference in the estimated number of victimization incidents, or, well, anything you like really! Marginal effects are a very powerful technique by which we can understand model results and cover a variety of specific methods to calculate quantities we might be interested in form a fitted model (see https://marginaleffects.com/). 

The fundamentals of all these methods are the same, but the details differ. I really recommend working through the guides at marginaleffects.com to get a sense of what is possible. Here we will work through a simple example. This is a very simple set-up to the clarify example we used, although with marginaleffects the focus is more on calculating predictions from the model (whereas in our example we converted coefficient values rather than predictions), and clarify focuses on simulation to express model uncertainty.


[^8:] The 'marginal' terminology is very confusing.




## Limits to simulation approaches

But remember that our model is wrong!! (Greenland) This approach incorporates the uncertainty *as expressed by your model's standard errors*. This still makes a load of assumptions - basically that we've fit the right model and that we had the right data. In more complex cases - like most real-world research - we might be fitting a much more complicated statistical model with non-linear effects and interaction terms and all sorts. As we'll discuss in the next section, we might also want to cast a critical eye over the data that we are analysing before drawing our conclusions.

We still need to bear in mind the generative story for our - data is our uncertainty due to survey sampling? What population does a victimization divide relate to?


# Practical

- Fit a regression model, simulate victimization divides with {clarify}

# Resources
