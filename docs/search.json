[
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\nUntil now!\nA new change here.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.0.1 About you\nI assume that you are familiar with the general linear model (although we will have a refresher) - my aim is not to cover things that you will find in a general undergraduate or postgrad methods course on statistical modelling, but focus on a set of illustratons of specific problems that can arise in some criminological contexts. I’ve tried to pick examples that are both specific enough to be illuminating, but general enough that the techniques I’ll desribe (of say simulating quantities of interest from your fitted model) are generally relevant. No guarantees though. Hopefully act as inspiration when grappling with your own analytical problems.\nThe thread linking all this issues that arise when fitting models to criminological data is that they need a solid understanding of how the data you are analysing came to exist. This data provenance influences the models we fit and how we should interpret their results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Atak, Kıvanç. 2020. “Beyond the Western Crime Drop:\nViolence, Property Offences, and the State in\nTurkey 19902016.” International\nJournal of Law, Crime and Justice 60 (March): 100373. https://doi.org/10.1016/j.ijlcj.2019.100373.\n\n\nBuil-Gil, David, Ian Brunton-Smith, Jose Pina-Sánchez, and Alexandru\nCernat. 2022. “Comparing Measurements of Violent Crime in Local\nCommunities: A Case Study in Islington,\nLondon.” Police Practice and Research 23\n(4): 489–506. https://doi.org/10.1080/15614263.2022.2047047.\n\n\nCarr-Hill, Roy. 2013. “Missing Millions and\nMeasuring Development Progress.” World\nDevelopment 46 (June): 30–44. https://doi.org/10.1016/j.worlddev.2012.12.017.\n\n\nGaebler, Johann, William Cai, Guillaume Basse, Ravi Shroff, Sharad Goel,\nand Jennifer Hill. 2022. “A Causal Framework for\nObservational Studies of\nDiscrimination.” Statistics and Public\nPolicy 9 (1): 26–48. https://doi.org/10.1080/2330443X.2021.2024778.\n\n\nGreenland, Sander. 2014. “Sensitivity Analysis and\nBias Analysis.” In Handbook of\nEpidemiology, edited by Wolfgang Ahrens and Iris\nPigeot, 685–706. New York, NY: Springer. https://doi.org/10.1007/978-0-387-09834-0_60.\n\n\nKnox, Dean, Will Lowe, and Jonathan Mummolo. 2020. “Administrative\nRecords Mask Racially Biased Policing.” American\nPolitical Science Review 114 (3): 619–37. https://doi.org/10.1017/S0003055420000039.\n\n\nScottish Women’s Aid. 2021. “Response to the Consultation on the\nScottish Crime and Justice Survey\n(SCJS), December 2021.”\nhttps://womensaid.scot/wp-content/uploads/2022/04/SWA-response-to-Scottish-Crime-and-Justice-Survey-consultation.pdf.\n\n\nSimes, Jessica T. 2021. Punishing Places: The\nGeography of Mass Imprisonment. Univ of\nCalifornia Press.\n\n\nSimes, Jessica T. 2018. “Place and Punishment:\nThe Spatial Context of Mass\nIncarceration.” Journal of Quantitative\nCriminology 34 (2): 513–33. https://doi.org/10.1007/s10940-017-9344-y.\n\n\nSimes, Jessica T., Brenden Beck, and John M. Eason. 2023.\n“Policing, Punishment, and Place:\nSpatial-Contextual Analyses of the Criminal Legal\nSystem.” Annual Review of Sociology 49 (1):\n221–40. https://doi.org/10.1146/annurev-soc-031021-035328.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Methods for Criminology",
    "section": "",
    "text": "Preface\nThis website contains the notes and R code for the NCRM workshop ‘Statistical Methods for Criminology’ which will run in September 2024.\nIt is very work in progress.\n\n\nAbout\nThis website contains the notes for the course. This is a combination of theoretical discussion and worked examples of fitting statistical models to criminological data and the issues that can arise when doing so.\nThe worked examples use ode in the R statistical language.\nThis website was built with Quarto using RStudio and is rendered with Github Pages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "types-of-data.html",
    "href": "types-of-data.html",
    "title": "2  Types of criminological data",
    "section": "",
    "text": "2.1 Who cares about data provenance?\nTo conduct a useful analysis - the kind that might help you understand the real world - you need to understand how your data came about. As we’ll see, whether you are analysing a survey or police recorded crime data or convictions data or something else matters in how you conduct your analysis and interpret the results. You need to know the ‘generative story’1 about your data to analyse it properly.\nWhen working with criminological data we often know quite a lot about how the data (or if you like, the numbers in the spreadsheet in front of you) came to exist. Using this information - information that might be in metadata, or based on our theoretical knowledge of the world - can help us do better data analysis.\nHow exactly we incorporate this information will depend a lot on what data we’re working with and what the goals of our analysis are. There are no general solutions to what information we should incorporate and how we might do this, but in this workshop we’ll cover some scenarios that can arise in criminological research.\n(Keay and Towers 2024, p228–229)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "intro.html#types-of-criminological-data",
    "href": "intro.html#types-of-criminological-data",
    "title": "1  Introduction",
    "section": "1.1 Types of criminological data",
    "text": "1.1 Types of criminological data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-general-linear-model",
    "href": "intro.html#the-general-linear-model",
    "title": "1  Introduction",
    "section": "1.2 The general linear model",
    "text": "1.2 The general linear model",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#measurement-error-and-selection-bias",
    "href": "intro.html#measurement-error-and-selection-bias",
    "title": "1  Introduction",
    "section": "1.3 Measurement error and selection bias",
    "text": "1.3 Measurement error and selection bias",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#ethics-of-working-with-criminological-data",
    "href": "intro.html#ethics-of-working-with-criminological-data",
    "title": "1  Introduction",
    "section": "1.5 Ethics of working with criminological data",
    "text": "1.5 Ethics of working with criminological data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#how-does-an-event-become-a-crime-statistic",
    "href": "types-of-data.html#how-does-an-event-become-a-crime-statistic",
    "title": "3  Types of criminological data",
    "section": "3.3 How does an event become a crime statistic?",
    "text": "3.3 How does an event become a crime statistic?\n[add link to SG flow diagram]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#how-does-an-incident-end-up-as-a-court-record",
    "href": "types-of-data.html#how-does-an-incident-end-up-as-a-court-record",
    "title": "3  Types of criminological data",
    "section": "3.4 How does an incident end up as a court record?",
    "text": "3.4 How does an incident end up as a court record?\nCrime​\nArrest and charge​\nCourt​\nSentencing​",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#how-does-an-incident-get-logged-as-a-police-stop",
    "href": "types-of-data.html#how-does-an-incident-get-logged-as-a-police-stop",
    "title": "3  Types of criminological data",
    "section": "3.5 How does an incident get logged as a police stop?",
    "text": "3.5 How does an incident get logged as a police stop?\n(Knox and Mummolo)\n\nObservation\nEncouter\nStop",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#key-dilemma",
    "href": "types-of-data.html#key-dilemma",
    "title": "2  Types of criminological data",
    "section": "2.3 Key dilemma",
    "text": "2.3 Key dilemma\nDo official crime statistics reflect ‘behavioural’ trends in crime, or just the actions of the justice system?​(Kitsuse and Cicourel, 1963)\nHow you see this may depend on your theoretical persuasion (Simes, Beck, and Eason 2023)\nSometimes this is less of a dilemma when you have a comparable victim survey and you are interested in overall levels of crime (Buil-Gil et al. 2022). However, this is a challenge “outside consolidated western democracies where the range of available data is often limited” (Atak 2020)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#survey-data",
    "href": "types-of-data.html#survey-data",
    "title": "2  Types of criminological data",
    "section": "2.3 Survey data",
    "text": "2.3 Survey data\nIn response to known issues with measuring levels of crime with administrative data, since the 1980s criminologists in some parts of the world have been surveying the public to ask about their levels of victimization.\nScottish Crime and Justice Survey​\nCrime Survey for England and Wales​\nEquivalents in other countries, primarily in Western Europe, North America and Australasia ​\nSmaller geographical scale surveys such as The Mayor’s Office for Policing And Crime (MOPAC) Survey in London",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#victimization-surveys",
    "href": "types-of-data.html#victimization-surveys",
    "title": "2  Types of criminological data",
    "section": "2.4 Victimization surveys",
    "text": "2.4 Victimization surveys\nTypically ask people about their experiences of victimization in the last year​\nCan measure crime that isn’t reported to the police​\nUsually don’t ask about people’s offending behaviour​\nGood for measuring common crimes, bad for measuring rare crimes\n\n2.4.1 Answering our key questions\n\n\n2.4.2 How does a person get selected for the SCJS?\nRandom sample of 12,000ish postcodes in Scotland are contacted to take part​(for 2020/21 survey it was 12,681 addresses)\nFor those who agree, one adult (age 16 or over) in each household is randomly selected for interview​\nTherefore, by design no:​\n\nChildren​\nHomeless people​\nPeople living in communal establishments (e.g. students, people in prison)\n\nThere may also be limitations on who ends up in a survey dataset even if they are included in the design. With a focus on International Development, Carr-Hill says that “in practice, household surveys typically under-represent: those in fragile, disjointed households; slum populations and areas posing security risks.” (Carr-Hill 2013). The extent to which these factors may affect any particular data collection exercise may vary from having little to extreme impact (for example, research being conduct in a conflict zone). It is up to the researcher to determine the likely extent of factors such as these in their own analysis.\n(Not going to talk about things like measurment equivalence in internaitonal surveys although that is a thing!)\n\n2.4.2.1 Who is excluded?\nFor SCJS to be a measure of crime for all adults, not just all adults in private households, we need to make a key assumption that “the subset of the adult population not captured in the SCJS experience the same level of victimisation as adults in the household resident population”​\nBut this excludes people, students, people in prison, and those living in refuges. However, “Domestic abuse is the main cause of women’s homelessness in Scotland… All women living in Women’s Aid refuges have experienced domestic abuse and many will have experienced other forms of crime”\n(Scottish Women’s Aid 2021)\n“The self-completion questionnaire on partner abuse and the questionnaire on sexual violence are the last part of an already long interview. The 2019/2020 technical report notes that “ran out of time” accounts for 34.2% (the largest proportion) of reasons for not doing the selfcompletion part of the SCJS.”\nThis means that students are excluded from estimates of domestic abuse in Scotland.\nIf you want to fit a statistical model to SCJS to understand partner abuse you need to be aware of these issues and possible bias (in terms of non-representativeness that may arise)\nImportantly, this is not a sample size issue - if you had all the data on the people in the sampling frame then you still wouldn’t find out anything about students because they are excluded from the survey by design.\n\n\n\n2.4.3 Implications of the generative story for modelling\n\n2.4.3.1 Accounting for survey design in analysis\nCrime surveys usually come with sampling weights (see https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/). This adjusts for the non-random selection of participants into the survey. It’s useful to think about the distinction between your survey and the population. If you want to calculate descriptive statistics from the survey that are accurate for the target population, you should use the weights. There are other occasions when you might want to know the unweighted/base sample size (e.g. to avoid presenting results that are based on a small number of responses). Different authorities have different perspectives on whether you should use sampling weights when fitting a statistical model. For example, Understanding Society say that you should definitely use sampling weights when analysing that dataset (https://www.understandingsociety.ac.uk/documentation/mainstage/user-guides/main-survey-user-guide/why-use-weights). Economists at the world bank say ‘yes when doing descriptive research and it depends when doing ’causal inference’” (https://www.nber.org/papers/w18859). As ever, the appropriate strategy will depend on the data you are working with and what question you are trying to answer. The best place to start is to consult the documentation for the survey you are using and see its description of the weights provided. For example, in the Scottish Crime and Justice survey they are different weights for individuals, households and incidents.\n\n\n2.4.3.2 What’s the population of interest?\nAgain, weights will only help adjust the data you see towards the target population in the survey design frame. Weighting cannot adjust for populations who are excluded from the survey by design.\nThere are not, as far as I know, standard recommendations for analytical ‘fixes’ for this problem. It may be that a researcher’s best option is to be clear about the limitations of their study and inferences that can be drawn about the whole population from their study.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#how-does-a-person-get-selected-for-the-scjs",
    "href": "types-of-data.html#how-does-a-person-get-selected-for-the-scjs",
    "title": "2  Types of criminological data",
    "section": "2.5 How does a person get selected for the SCJS?",
    "text": "2.5 How does a person get selected for the SCJS?\nRandom sample of 12,000ish postcodes in Scotland are contacted to take part​(for 2020/21 survey it was 12,681 addresses)\nFor those who agree, one adult (age 16 or over) in each household is randomly selected for interview​\nTherefore, by design no:​\n\nChildren​\nHomeless people​\nPeople living in communal establishments (e.g. students, people in prison)\n\nThere may also be limitations on who ends up in a survey dataset even if they are included in the design. With a focus on International Development, Carr-Hill says that “in practice, household surveys typically under-represent: those in fragile, disjointed households; slum populations and areas posing security risks.” (Carr-Hill 2013). The extent to which these factors may affect any particular data collection exercise may vary from having little to extreme impact (for example, research being conduct in a conflict zone). It is up to the researcher to determine the likely extent of factors such as these in their own analysis.\n(Not going to talk about things like measurment equivalence in internaitonal surveys although that is a thing!)\n\n2.5.0.1 Who is excluded?\nFor SCJS to be a measure of crime for all adults, not just all adults in private households, we need to make a key assumption that “the subset of the adult population not captured in the SCJS experience the same level of victimisation as adults in the household resident population”​\nBut this excludes people, students, people in prison, and those living in refuges. However, “Domestic abuse is the main cause of women’s homelessness in Scotland… All women living in Women’s Aid refuges have experienced domestic abuse and many will have experienced other forms of crime”\n(Scottish Women’s Aid 2021)\n“The self-completion questionnaire on partner abuse and the questionnaire on sexual violence are the last part of an already long interview. The 2019/2020 technical report notes that “ran out of time” accounts for 34.2% (the largest proportion) of reasons for not doing the selfcompletion part of the SCJS.”\nThis means that students are excluded from estimates of domestic abuse in Scotland.\nIf you want to fit a statistical model to SCJS to understand partner abuse you need to be aware of these issues and possible bias (in terms of non-representativeness that may arise)\nImportantly, this is not a sample size issue - if you had all the data on the people in the sampling frame then you still wouldn’t find out anything about students because they are excluded from the survey by design.\n\n\n2.5.1 Accounting for survey design in analysis\nCrime surveys usually come with sampling weights (see https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/). This adjusts for the non-random selection of participants into the survey. It’s useful to think about the distinction between your survey and the population. If you want to calculate descriptive statistics from the survey that are accurate for the target population, you should use the weights. There are other occasions when you might want to know the unweighted/base sample size (e.g. to avoid presenting results that are based on a small number of responses). Different authorities have different perspectives on whether you should use sampling weights when fitting a statistical model. For example, Understanding Society say that you should definitely use sampling weights when analysing that dataset (https://www.understandingsociety.ac.uk/documentation/mainstage/user-guides/main-survey-user-guide/why-use-weights). Economists at the world bank say ‘yes when doing descriptive research and it depends when doing ’causal inference’” (https://www.nber.org/papers/w18859). As ever, the appropriate strategy will depend on the data you are working with and what question you are trying to answer. The best place to start is to consult the documentation for the survey you are using and see its description of the weights provided. For example, in the Scottish Crime and Justice survey they are different weights for individuals, households and incidents.\nAgain, weights will only help adjust the data you see towards the target population in the survey design frame. Weighting cannot adjust for populations who are excluded from the survey by design.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#who-is-excluded",
    "href": "types-of-data.html#who-is-excluded",
    "title": "3  Types of criminological data",
    "section": "3.10 Who is excluded?",
    "text": "3.10 Who is excluded?\nFor SCJS to be a measure of crime for all adults, not just all adults in private households, we need to make a key assumption that “the subset of the adult population not captured in the SCJS experience the same level of victimisation as adults in the household resident population”​\nBut this excludes people, students, people in prison, and those living in refuges​\n“Domestic abuse is the main cause of women’s homelessness in Scotland… All women living in Women’s Aid refuges have experienced domestic abuse and many will have experienced other forms of crime”\n\nFrom Handbook chapter - falling sample size, falling response rate and falling victimization all mean less ‘power’ in more recent SCJS sweeps\n\n“The reduction in survey response rates is a phenomenon that has been observed internationally (de Leeuw, Hox and Luiten, 2018), and there are particular concerns that it is the most marginalized in society (often the most prone to victimization) who are no longer responding to surveys (Savage and Burrows, 2009). Again, taking Scotland as an example, valid response rates to the SCJS fell from 71% in 2008/09 to 63% in 2019/20 (Saunders et al, 2021), with a urther reduction (following the Coronavirus pandemic) to just 47.3% in 2021/22 - the lowest response rate for any SCJS sweep since 2008/09 (Scottish Government 2023a). This issue has been compounded by reductions in the valid sample size for successive sweeps of the SCJS, which has reduced from 16,000 in 2008/09 to just 5,500 in the sweeps conducted since 2016/1714, largely in an effort to cut costs. Factoring in the large reduction in the prevalence of victimization (mentioned above), the stark reality is that the number of victims interviewed as part of the Scottish survey reduced from 2,786 in 2008/09 to just 639 in 2021/22. So, whilst the prevalence of crime fell by just over 50% (or in absolute terms, a fall of 10.4 percentage points), the number of survey respondents who had experienced at least one incident of crime has fallen by an astonishing 77%.”\nYou need to evaluate how useful a data source is in the context of your particular research question. e.g. you might be able to make an overall assessment of victimization for some social group but not look at a trend for that group, whilst you can look at trends for other groups. I guess this is curse of dimensionality. Or you might need a different model for some groups versus others/have to assume more.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#make-sure-the-data-is-right",
    "href": "types-of-data.html#make-sure-the-data-is-right",
    "title": "3  Types of criminological data",
    "section": "3.11 Make sure the data is right!!",
    "text": "3.11 Make sure the data is right!!\nhttps://x.com/jkangbrown/status/1790416534776562157",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html",
    "href": "general-linear-model.html",
    "title": "3  The general linear model",
    "section": "",
    "text": "3.1 Large worlds and small worlds\nRichard McElreath (McElreath 2020) talks about the ‘small world’ of the model and the ‘large world’ that we actually live in. Our spreadsheets and coefficients can only summarize the small world for us, and omit some of the complexity of the large world - all models are wrong, but some are useful and all that. To some extent, this is good!\nAnd all we want to do is provide summaries of the the numbers in our spreadsheets we have no problem. But as soon as we want to understand the large world we can run in to problems if all we focus on are the rows and columns in front of us.\nLater on we’ll consider how we can bring our understanding of the large world - for example, how an incident becomes a crime - to bear during statistical modelling. In this session we’ll overview the most common[^3] way in which criminologists understand the ‘small world’ of their spreadsheets: the general linear model.\n[^3:] Probably?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html",
    "href": "selection-measurement.html",
    "title": "5  The data don’t speak for themselves, part one: Measurement error, selection and confounding",
    "section": "",
    "text": "5.1 Measurement error\nMeasurement error is the gap between what we are conceptually interested in and the way that this concept is recorded in our spreadsheet.\nMeasurement error can be present in the outcome variable, a focal independent variable or in a control variable, each of which can have different implications for analysis. The standard linear model which we have just been discussing assumes that all variables are measured perfectly (or in other words, with no error). If it is in the outcome variable it may not bias your regression coefficients at all but just impact the precision of your results (meaning that they are less likely to be statistically significant). Measurement error in your key independent variable may bias your regression coefficients downwards, meaning that your results are valid and in reality there is a stronger association between predictor and outcome that you have observed. If there is more noise in a control than in a key independent variable, measurement error in the control variable may lead to ‘under controlling’ - and finding statistically significant coefficients where there are none. Other than in simple scenarios we may not know what impact it is having.\nAdd discussion and simulations from https://gist.github.com/benmatthewsed/4aade247c6645698090c1b3949c2b3c7?\nIn practice we know criminological data are likely to be measured with some degree of error. For example, we know that police recorded crime data is not a perfect measure of the amount of crime ‘out there’ in society. As we have discussed already, not all crimes are reported to the police, not all incidents which are reported are recorded as crimes and so on.\nOverview video: see https://www.youtube.com/watch?v=B0ZpenVs8oI",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The data don't speak for themselves, part one: Measurement error, selection and confounding</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#solutions",
    "href": "selection-measurement.html#solutions",
    "title": "5  The data don’t speak for themselves, part one: Measurement error, selection and confounding",
    "section": "5.3 Solutions?",
    "text": "5.3 Solutions?\nKnox et al. (2020) suggest some technical fixes, but emphasise that - if we are interested in using statistical models to identify causal relationships there is no general solution that can guarantee that coefficients in a regression model will have valid causal interpretations based on administrative data derived from police records. The key thing is thinking through the process by which the dataset was constructed, and conveying this to your reader.\nmaybe get the students to play about with the data in the dataverse?https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KFQOCV",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The data don't speak for themselves, part one: Measurement error, selection and confounding</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#probabilistic-bias-analysis",
    "href": "selection-measurement.html#probabilistic-bias-analysis",
    "title": "4  Selection effects, bias and so on",
    "section": "4.4 Probabilistic Bias Analysis",
    "text": "4.4 Probabilistic Bias Analysis\nWhat Knox et al. suggest is part of a set of approaches called Probabilistic Bias Analysis. This includes things like understanding selection bias, unmeasured confounding and misclassification.\nThere are whole books written on this topic! So no general solutions here.\n\n4.4.1 Selection bias\n(Greenland 2014)\nWe only see people in our study if they meet some criterion (in other words, where S = 1), but we want to see everybody (i.e. people for whom S = 0)\n“But because we see only the relation of X to Y conditional on selection (S D 1), we must impute the unconditional relation of X to Y using probabilities of selection given what we do see (X and Y given S D 1).” In the Knox and colleagues example, this would require knowing the numbers of people who were observed by police but not stopped, in order to calculate the probabilities of selection into the stop dataset. However, we don’t know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset would know this.\nEven if we do know this for our particular dataset, there is no guarantee that selection into the data would hold in every case that we might want to generalize our results to. As such we’d need to consider how differences between the study populations may have affected response and selection (e.g. would selection probabilities from a US study map onto a study in Manchester? How about one in Glasgow?)\n\n\n4.4.2 Confounding\nConfounding describes a situation where there’s something that we know affects the outcome we’re interested in and/or our independent variables, but we don’t have a measure for this. In criminology, we might have measures of police stops but not offending (.e.g from self-reports). Offending is likely to be a big - but not the only driver - of whether a person has contact with the police.\n\nbecause we do not see U, we must impute its values using probabilities (bets) about the values of U given what we do see (again, X and Y ).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selection effects, bias and so on</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#in-general",
    "href": "selection-measurement.html#in-general",
    "title": "5  Selection effects, bias and so on",
    "section": "5.4 In general",
    "text": "5.4 In general\nNeed to make assumptions about the magnitude of the bias to implement any of the technical fixes. Fine. But where does this information come from?\n“The preceding approach assumes that U is a known confounder (e.g., a smoking indicator) that was unmeasured in the study in question but has been previously identified and subject to study in relation to disease if not exposure. If instead U represents an unspecified, unknown confounder, then the entire sensitivity exercise will remain far more speculative. Nonetheless, decomposition of the bias factor can still be successful in demonstrating that only implausibly strong confounder or selection effects can account for a strong observed association. Cornfield et al. (1959) is considered a landmark study in which such an approach was used to examine claims that the smoking-lung cancer relation might be attributable to confounding”\n\nso this is based on the idea tat we can identify an “implausibly strong” confounder, which is reasonable.\nOne approach is to pick a bunch of possible bias parameters and test to see if results are robust to all of them.\n\n“Despite these considerations, there is no basis for mandating a bias analysis of every study or even most studies. For example, bias analysis is superfluous when conventional intervals show that no useful conclusion could be drawn from the study even if it were perfect apart from random error. More generally, rather than providing a bias analysis, a study may provide greater service by refraining from inference; instead it can focus on carefully reporting its design, conduct, and data in great detail to facilitate pooling and meta-analysis (Greenland et al. 2004). Inferences are best based on a more complete account of evidence than can be provided in a single study report, and thus the effort of bias analysis is more justifiable in research synthesis (Turner et al. 2009; Welton et al. 2009). Even there, bias analysis becomes essential only when doing risk assessment or when authors claim to offer near-definitive conclusions.” (Greenland 2014, p703)\n… so you only need to bother with this stuff if you ‘claim to offer near-definitive conclusions’. Is your study likely to contribute to a meta analysis? Or in other words, when you are moving between the small world and the large world.\nMeans humility when making policy recommendations!!\nIs the small world enough? How do we talk about this? e.g. if all we’ve done is analysed convictions data do we talk about offending, or just conviction?\n\n\n\n\nBlackwell, Matthew, James Honaker, and Gary King. 2017. “A Unified Approach to Measurement Error and Missing Data: Overview and Applications.” Sociological Methods & Research 46 (3): 303–41. https://doi.org/10.1177/0049124115585360.\n\n\nBushway, Shawn, Brian D. Johnson, and Lee Ann Slocum. 2007. “Is the Magic Still There? The Use of the Heckman Two-Step Correction for Selection Bias in Criminology.” Journal of Quantitative Criminology 23 (2): 151–78. https://doi.org/10.1007/s10940-007-9024-4.\n\n\nGaebler, Johann, William Cai, Guillaume Basse, Ravi Shroff, Sharad Goel, and Jennifer Hill. 2022. “A Causal Framework for Observational Studies of Discrimination.” Statistics and Public Policy 9 (1): 26–48. https://doi.org/10.1080/2330443X.2021.2024778.\n\n\nGreenland, Sander. 2014. “Sensitivity Analysis and Bias Analysis.” In Handbook of Epidemiology, edited by Wolfgang Ahrens and Iris Pigeot, 685–706. New York, NY: Springer. https://doi.org/10.1007/978-0-387-09834-0_60.\n\n\nPina-Sánchez, Jose, and John Paul Gosling. 2020. “Tackling Selection Bias in Sentencing Data Analysis: A New Approach Based on a Scale of Severity.” Quality & Quantity 54 (3): 1047–73. https://doi.org/10.1007/s11135-020-00973-z.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selection effects, bias and so on</span>"
    ]
  },
  {
    "objectID": "intro.html#using-models-to-describe-quantities-of-interest",
    "href": "intro.html#using-models-to-describe-quantities-of-interest",
    "title": "1  Introduction",
    "section": "1.4 Using models to describe quantities of interest",
    "text": "1.4 Using models to describe quantities of interest",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#assumptions-and-p-values",
    "href": "general-linear-model.html#assumptions-and-p-values",
    "title": "3  The general linear model",
    "section": "3.5 Assumptions and p-values",
    "text": "3.5 Assumptions and p-values\nhttps://bristoluniversitypressdigital.com/edcollchap/book/9781529232073/ch014.xml\nFocus on effect size measures instead?\nThink about populations hard (https://benmatthewsed.github.io/what_to_do_odds_ratios/what_to_do_odds_ratios.html#/title-slide)\n\n3.5.1 An example of thinking about populations and generalization\nThis question came from the Policing the Pandemic in Scotland project with excellent colleagues Dr Vicky Gorton and Prof Susan McVie\nWe linked data on all (well, most) fines received for breaching the Covid regulations in Scotland between 27 March 2020 to 31 May 2021 with information on recipients’ health (service use) and (some) social circumstances (I’m not going to go into detail about this)\nWe also have the same information on a comparison group of matched controls (matched by age, sex, Local Authority and SIMD decile)\nWe want to know if people with more ‘vulnerability’ (read - health service use) are more likely than others to have received a Covid fine (FPN)\nHaving done all this, I actually don’t think we’ll use this in our paper. Thinking hard about the population that we’re interested in made me wonder…\n… and what’s wrong with an odds ratio of 35 anyway?\nThis is an accurate description of our dataset!\nIf the problem is that we don’t think a result this extreme would generalize to another ‘sample’ from the sample population - with close to every person who received an FPN do we even have any issues of generalizability (we have basically 100% of the relevant people, minus matching error)?\nInstead of generalizability, I think we have either a massive issue with transportability/external validity (Degtiar and Rose 2023), or we have no issue at all\nIt seems nonsensical to suggest that these results would apply to another country during Covid or another pandemic (countries were very different in their responses)\nThe results for Lockdown One in Scotland don’t even generalize to Lockdown Two - we show that in our analysis!\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#draw-from-pina-sanchez",
    "href": "selection-measurement.html#draw-from-pina-sanchez",
    "title": "Selection effects, bias and so on",
    "section": "Draw from Pina sanchez",
    "text": "Draw from Pina sanchez\nhttps://josepinasanchez.uk/wp-content/uploads/2021/12/rmef_measurement-error_b.pdf\nfrom https://josepinasanchez.uk/short-courses/",
    "crumbs": [
      "Selection effects, bias and so on"
    ]
  },
  {
    "objectID": "selection-measurement.html#measurement-error",
    "href": "selection-measurement.html#measurement-error",
    "title": "5  The data don’t speak for themselves, part one: Measurement error, selection and confounding",
    "section": "",
    "text": "5.1.1 Example One: Measurement error in police recorded crime data\n\nFrom our discussion in Session One, we know that crime data recorded by the police are not a complete record of all crimes experienced in society in a given period - only crimes which are reported and recorded make it into recorded crime data.\nSo if we are interested in, for example, how many crimes there were in Scotland in 2023, the number of crimes recorded by police is likely to be an under count.\nThe many years of work comparing crimes recorded to the police with victimization surveys - the ‘dark figure of crime’ - attests to this.\n\n\n\n5.1.2 Example Two: Measurement error in victimizaton survey data\n\nHistorically, national victimization surveys (such as the SCJS and CSEW) capped the number of victim forms that victims could complete. In 2019, ONS said that “Since the survey began in 1981, “repeat” incidents have been limited to a total of 5. Historically, including a maximum of 5 repeat incidents for any individual victim had proven to be an effective way of reducing the effects of sample variability from year to year. This approach enabled the publication of incident rates that were not subject to large fluctuation between survey years. This approach yields a more reliable picture of changes in victimisations over time once high order repeat victimisations were treated in this way.” (https://doc.ukdataservice.ac.uk/doc/7280/mrdoc/pdf/7280_csew_improving_victimisation_estimates_2019.pdf)\nHowever, it also means that people who experienced more than five incidents of a particular ‘series’ crime type did not have their data accurately recorded\nThis was particularly important for women’s reporting of violent victimization (Walby et al. 2015) - women who experienced domestic violence may well report more than five repeat incidents of victimization in a given year. A second measurement error issue came from the ‘97 code’ - the option to report the number of incidents experienced as ‘96/too many to count’. Instead, based on the domestic violence literature Walby and colleagues propose using an estimate of 60 incidents for those who report the 97 code.\nSometimes it’s possible to use uncapped data\n\n\n\n5.1.3 What to do about it?\nIt depends on the context.\nOne way to approach this, if we know or can reasonably approximate the model for the measurement error then we can use Bayesian methods to jointly estimate a model for our observations and the measurement error. This is easier said than done! Bayesian methods are extremely flexible and allow the researcher to model to specify a model for the measurement error which is estimated jointly with the model for their outcome. This relies on knowing what the right model for the measurement error should be. It also relies on fitting a Bayesian model to your data which comes with its own set of challenges.\nPina-Sanchez has written about measurement error in crime.\nPina Sanchez gave a workshop on Bayesian adjustments for measurement error, and you can find the materials here: https://josepinasanchez.uk/short-courses/\nhttps://josepinasanchez.uk/wp-content/uploads/2021/12/rmef_measurement-error_b.pdf\nfrom https://josepinasanchez.uk/short-courses/\n\n\n5.1.4 SIMEX\nA less involved option involves assessment measurement error through simulation. The process here is to fit the model to the data as observed and try to figure out what the coefficients would be if there was no measurement error in the data. The simex R package lets you simulate new versions of your dataset with more and more measurement error. By looking to see how your coefficients change after re-fitting your model with increasing measurement error you can project backwards to what the coefficients would be with zero measurement error. Neat!\nHowever, SIMEX works best when there is only one variable with measurement error in the analysis, and can be more difficult to compute when there are multiple variables with measurement error (Blackwell, Honaker, and King 2017)\n\n\n5.1.5 rcme\nPina-Sanchez and colleagues have written an R package that can conduct sensitivity analysis for some types of measurement error common to working with police recorded crime.\n\nWork through their example here? https://osf.io/preprints/socarxiv/sbc8w\n\nOpen questions - what about survey data? Models other than linear models? What about measurement error in independent variables?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The data don't speak for themselves, part one: Measurement error, selection and confounding</span>"
    ]
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "4  The data don’t speak for themselves, part two: Presenting results",
    "section": "",
    "text": "4.0.1 Quantities of interest",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The data don't speak for themselves, part two: Presenting results</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#a-note-on-bespoke-models",
    "href": "general-linear-model.html#a-note-on-bespoke-models",
    "title": "3  The general linear model",
    "section": "3.6 A note on bespoke models",
    "text": "3.6 A note on bespoke models\nI am extremely partial to bespoke models. If you have the time I would recommend reading and watching all the Statistical Rethinking lectures - this gives an introduction into building your own models tailored to your specific data and problems. But that will take weeks and weeks and we are only here until five.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#geography",
    "href": "types-of-data.html#geography",
    "title": "2  Types of criminological data",
    "section": "2.2 Geography",
    "text": "2.2 Geography\nMostly scope is national, but not always​\nSome local surveys​(suchas MOPAC/Islington Crime Survey)\nCan get data on trends for [individual police forces in the UK] (https://data.police.uk/data/), or [victimization survey data aggreted to police division level within Scotland] (https://scotland.shinyapps.io/sg-scottish-crime-justice-survey/)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "6  Doing ethical quantitative criminology",
    "section": "",
    "text": "6.1 General ethical principles in social science research\nIn research ethics and governance there is a lot of discussion about informed consent, not disclosing personal information and so on.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Doing ethical quantitative criminology</span>"
    ]
  },
  {
    "objectID": "ethics.html#framing",
    "href": "ethics.html#framing",
    "title": "6  Ethics",
    "section": "6.2 Framing",
    "text": "6.2 Framing\nMore conceptually, it’s important to think about how we frame the results of any analysis.\nThree visualizations from Data Feminism\n\nlanguage use\nproviding necessary context?\ndeficit narrative\nthe description of your charts is theoretically informed",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#generative-stories-for-common-types-of-crime-data",
    "href": "types-of-data.html#generative-stories-for-common-types-of-crime-data",
    "title": "2  Types of criminological data",
    "section": "2.2 Generative stories for common types of crime data",
    "text": "2.2 Generative stories for common types of crime data\n\n2.2.1 How does an event become a crime statistic?\nIn Scotland there is a very nice discussion of how police recorded crime statistics are put together in the Scottish Government User Guide to Recorded Crime. We can think of this process as involving four main components:\n - Reporting: Most of the time incidents come to the attention of the police after a member of the public reports it. However, we know that not all incidents are reported to the police, and not all people are equally likely to report incidents if they are victims (Fohring 2014) - Fact-checking/investigation: Police need to collect initial information about an incident to establish whether a crime has occurred - Discretion/judgement: Even if the police believe a crime has occurred, not all incidents are followed up, for example if a victim chooses not to prosecute or provide details of the perpetrator (Aplin 2019). Maybe add discussion of (Hope 2023) here? - Applying crime counting rules: This might affect the type of crime an incident is recorded as, including whether it is classed as a hate crime. The current Scottish Crime Recording Standard is 550 pages long (!). I have not read it.\n\n\n2.2.2 Answering our key questions\n\n\n2.2.3 Implications of the generative story\n\n2.2.3.1 System or heaviour?\nBecause of all these processes there has long been a question about the extent to which official crime statistics reflect ‘behavioural’ trends in crime, or just the actions of the justice system (Kitsuse and Cicourel 1963; Hope 2023), and how you see this may depend on your theoretical persuasion (Simes, Beck, and Eason 2023). We’ll discuss some practical impacts this may have on analysis later in the workshop.\n\n\n2.2.3.2 Undercounting and the dark-figure\n\n\n2.2.3.3 Population or super-population?\n(Verlaan and Langton 2024) outline two possible approaches to conceptualizing police recorded crime data: a population or superpopulation approach.\nIn the population approach the data you observe for a particular time and place (e.g. Scotland in 2023) are all the data you could ever observe - you don’t need to fit statistical models to generalize from the data you observe to a wider population because there is no wider population. [^2] You have the data you have, and that’s that.\n[^2:] You might want to fit models for other reasons though.\nIn the superpopulation approach you are interesting in thinking about other ways the data you have may have arisen. Verlan and Langton (2024) give examples where you may think that data on crimes recorded by police in cities in Texas from 2023 were data drawn from the population of cities in the whole USA, or the whole of the USA, or the whole planet. Alternatively, data from cities in Texas from 2023 may be seen as a sample from cities in Texas from 2023 and 2024, or 2023-2030 and so on. Gelman (2011) takes this approach using an example from political science - you may have data on all seats in a legislature but want to predict what will happen to those same seats in future elections under “hypothetical alternative conditions”. Applying the same logic, statistical models could be used with police recorded crime data to ask ‘counterfactual’ questions about what crime rates in Dallas may have been if the poverty rate fell by 5%.\nVerlan and Langton list some possible problems using inferential statistcs/statistical models using administrative data.\n\nIt wrongly leads people to assume that their results are generalizable. This is often not done explicitly but implicitly (for example suggesting that results may apply in other jurisdictions)\nIt can lead people to undervalue actual observed differences in their data. For example, researchers may deny that an association between two variables exists in their dataset if it is not statistically significant\n\nHowever, there are arguments for want to express uncertainty in estimates derived from administrative data.\nD. Redelings et al. (2012) argue that confidence intervals should be reported even when describing statistics from the full population, especially if the results are to be used to make predictions or inform policy. Importantly, even if you are not be interested in prediction or policy-making, you can’t control how others will use your results. Confidence intervals can therefore provide useful information for others.\nSpiegelhalter (2005) makes a similar point when advocated for ‘funnel plots’ to show compare institutional performance. These graphical methods put observed rates in the context of how much variation in rates of e.g. reconviction between Local Authorities given the population size and number of people convicted see here. Here there is information on the whole population of people convicted and re-convicted within a given Local Authority, but analysts still want to account for the “variability of different sized populations” and in doing so “highlight whether there are differences that may be attributed to some other special cause”.\nGelman (2023) argues that the ‘correct’ standard error for a given analysis depends on what the researcher’s goal is. As we’ve seen, it also depends on the generative story for the data.\n[ take from slides about volatility]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#count-data",
    "href": "general-linear-model.html#count-data",
    "title": "3  The general linear model",
    "section": "3.3 Count data",
    "text": "3.3 Count data\nCount data are counts of things. That’s it! This means that they non-negative whole numbers. Count data are common in criminology when it comes to modelling crime - e.g. the number of crimes reported to the police, or the number of victimization incidents experienced by victims.\nWhilst we may see crime data be re-expressed as rates per 1,000 population, before this they are counts.\nThe foundational model for count data is the Poisson model:\n\\[\n\\begin{align*}\ny_i \\sim & {Poisson} (\\lambda) \\\\\n{log(\\lambda)} & = \\alpha + \\beta (x_i),\n\\end{align*}\n\\]\nNow there is only one parameter (lambda; \\(\\lambda\\)) that we are modelling, unlike with linear regression. This means that in Poisson models the mean and the variance are assumed to be the same (or put another way, that they are both direct functions of \\(\\lambda\\)).\nThe coefficients from Poisson models (which range from \\(-\\infty\\) to \\(\\infty\\)) are converted to be predicted counts that are non-negative integers via the \\(\\log\\) link function. This lets us have coefficients which can take any value (-2! 0.5!), but then convert these to counts as our count outcome demands.\nGraph of log here?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#why-not-just-lm",
    "href": "general-linear-model.html#why-not-just-lm",
    "title": "3  The general linear model",
    "section": "3.4 why not just lm?",
    "text": "3.4 why not just lm?\n\nYou can get negative predictions\nLM assumes constant variance - we probably want more variance for larger counts\n\n\n3.4.1 Specific problems with GLMs\n\n3.4.1.1 over-dispersion\n\npoisson assumes mean and variance are the same (in that there is only one term in the model, \\(\\lambda\\), which describes both the average and the variability around the average)\nso what? when in your data the variance exceeds the mean, you will get the wrong standard errors\nthis breaks basically all your inferences!\np-values are too optimistic\nExample of gun violence in Oakland: https://cao-94612.s3.amazonaws.com/documents/Oakland-Ceasefire-Evaluation-Final-Report-May-2019.pdf\n\n\n3.4.1.1.1 What do to? about over-dispersion\nIf all you care about is your standard errors you can use quasi-poisson (same point estimates as poisson but with ‘empirical’ SEs - similar to using robust standard errors). Francis et al. use this approach to modelling counts of victimization. In my experience it’s also faster than the standard alternative…\n… which is negative binomial binomial regression.\nThe negative binomial is a flavour of statistical model 1 for count data which has an extra “dispersion” parameter, which allows for over-dispersion. The poisson model can’t do this, because it has only one parameter (\\(\\lambda\\)) so the model has nowhere to ‘put’ any information about overdispersion.\nAt this point the details get a bit involved, but interested readers can see more in:\nHilbe (2014) for an accessible introduction and Hilbe (2011) for a deep-dive\nVer Hoef and Boveng (2007) give a blow-by-blow account of the differences between the two models in how they handle over-dispersion. But I would suggest that these distinctions depend on the kind of research question you are interested in. If all you care about is the mean for the two groups then you (might not care about over-dispersion at all)[https://www.statalist.org/forums/forum/general-stata-discussion/general/1570223-non-count-outcomes-and-use-of-poisson-regression?p=1570448#post1570448]. If you really care about modelling the extra dispersion in your negative binomial distribution (this isn’t very common but hey you might want to) then you can’t do this with a poisson model and you need something like negative binomial (see https://stats.stackexchange.com/a/568052)\n\n\n\n3.4.1.2 zero-inflation\ntake from Hilbe (distinct zeros? ZIP; generic solution; NB?)\n\nZIP can have different predictors for zeros than count part\nyou can also have different predictors for dispersion and rate parameter in NB if you want",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#logistic-regression",
    "href": "general-linear-model.html#logistic-regression",
    "title": "3  The general linear model",
    "section": "3.3 Logistic regression",
    "text": "3.3 Logistic regression\nFor logistic regression we have:\n\\[\n\\begin{align*}\ny_i \\sim & {Binomial} (n, p_i) \\\\\n{logit(p_i)} & = \\alpha + \\beta (x_i),\n\\end{align*}\n\\] for logistic regression, \\(n\\) = 1, and we are just interested in modelling \\(p_i\\).\nThese kinds of models are very common in the social sciences, including in criminology. We might want to model whether someone is a victim of crime or not, or whether the person has been convicted of a crime in a given year, or whether they are confident in the police or not… and so on and so on. The nice thing about this model formulation is that the \\({logit}\\) link function makes sure that all the probabilities the model estimates are between 0 and 1. Strictly speaking we don’t need to do this (see the linear probability model), but it’s nice to avoid results that obviously don’t make sense, like negative probabilities and such.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#intro-to-glm",
    "href": "general-linear-model.html#intro-to-glm",
    "title": "3  The general linear model",
    "section": "3.2 Intro to GLM",
    "text": "3.2 Intro to GLM\nIf you are taking this course I assume that you have some familiarity with linear models of some description. It is standard in quantitative methods training for social scientists to discuss the linear model, so I’ll only provide a brief refresher here before moving on to discuss models that are more typical in criminology.\nI should say that general linear models are complicated things, and it would be possible to spend months and months studying nothing but general linear models and their various extensions. Some specific flavours of model that I don’t cover, but which may be useful for your own work include: - Won’t include: mixed/multilevel/hierarchical/etc (see e.g. https://www.cmm.bris.ac.uk/lemma/) - Additive models (GAMs) (which are less common in criminology, but are very useful; https://noamross.github.io/gams-in-r-course/) - ‘bespoke’ models (very infrequently used in criminology to my knowledge, https://betanalpha.github.io/assets/case_studies/generative_modeling.html, but see e.g. https://josepinasanchez.uk/wp-content/uploads/2018/09/bsc-presentation.pdf)\nRecommended watching: https://www.youtube.com/watch?v=qbxNf4iqJPo&t=1215s\nIntroduction to LM\n\nStochastic (or random) component\nSystematic component\nLink function that converts between the parameter estimates and the form of the outcome (we’ll say more about this later)\n\n(I adapted this notation from Solomon Kurz https://bookdown.org/content/4857/god-spiked-the-integers.html#poisson-regression)\n\\[\n\\begin{align*}\ny_i \\sim & {Distribution} (\\theta_i, \\phi) \\\\\n{f(\\theta_i)} & = \\alpha + \\beta (x_i - \\bar x),\n\\end{align*}\n\\]\nwhere \\(\\theta_i\\) is the parameter of interest (e.g., the probability of 1 in a Binomial distribution) and \\(\\phi\\) is a placeholder for any other parameters necessary for the likelihood but not typically of primary substantive interest (e.g., \\(\\sigma\\) in conventional Gaussian models). The \\(f(\\cdot)\\) portion is the link function.\nFor the linear model, we have:\n\\[\n\\begin{align*}\ny_i \\sim & {Normal} (\\theta_i, \\sigma) \\\\\n{Identity(\\theta_i)} & = \\alpha + \\beta (x_i - \\bar x),\n\\end{align*}\n\\]\n\nan example here?\n\nOne key feature of a GLM is that you can get predictions for your outcome by adding up the coefficients for each element of your systematic component, and then applying the appropriate transformation through the link function.\n\n3.2.1 Logistic regression\nFor logistic regression we have:\n\\[\n\\begin{align*}\ny_i \\sim & {Binomial} (n, p_i) \\\\\n{logit(p_i)} & = \\alpha + \\beta (x_i),\n\\end{align*}\n\\] for logistic regression, \\(n\\) = 1, and we are just interested in modelling \\(p_i\\).\nThese kinds of models are very common in the social sciences, including in criminology. We might want to model whether someone is a victim of crime or not, or whether the person has been convicted of a crime in a given year, or whether they are confident in the police or not… and so on and so on. The nice thing about this model formulation is that the \\({logit}\\) link function makes sure that all the probabilities the model estimates are between 0 and 1. Strictly speaking we don’t need to do this (see the linear probability model), but it’s nice to avoid results that obviously don’t make sense, like negative probabilities and such.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#selection-effects",
    "href": "selection-measurement.html#selection-effects",
    "title": "5  The data don’t speak for themselves, part one: Measurement error, selection and confounding",
    "section": "5.2 Selection effects",
    "text": "5.2 Selection effects\nSelection bias arises when there are people who we would have liked to observe in our study but we don’t observe them, and this lack of observation is related to their characteristics. Put another way, we can think of selection bias as affecting the rows of our dataset - there are some rows that are missing that we would like to see.\n(Greenland 2014)\nWe only see people in our study if they meet some criterion (in other words, where S = 1), but we want to see everybody (i.e. also people for whom S = 0)\n“But because we see only the relation of X to Y conditional on selection (S D 1), we must impute the unconditional relation of X to Y using probabilities of selection given what we do see (X and Y given S D 1).”\nBushway (2007) gives an example from sentencing research: say we want to investigate racial discrimination in sentencing. A common approach has been to focus on whether people from different ethnic backgrounds receive different lengths of prison sentence, after statistically adjusting for other factors such as the offence type, age at time of offence and so on (Pina-Sánchez and Gosling 2020). However this creates a selection bias problem - not all convictions lead to custodial sentences, and so focusing only on custodial sentences means that the analyst is focusing on discrimination “conditional on being selected into the incarcerated population” (Bushway, Johnson, and Slocum 2007). Any results derived would then not relate to all convicted people.\nAdd more from (https://www-jstor-org.ezproxy-s2.stir.ac.uk/stable/2095230?seq=8)?\nIn the Knox and colleagues example, this would require knowing the numbers of people who were observed by police but not stopped, in order to calculate the probabilities of selection into the stop dataset. However, we don’t know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset would know this.\nEven if we do know this for our particular dataset, there is no guarantee that selection into the data would hold in every case that we might want to generalize our results to. As such we’d need to consider how differences between the study populations may have affected response and selection (e.g. would selection probabilities from a US study map onto a study in Manchester? How about one in Glasgow?)\nSome links on where to read more about causal inference? Maybe here\nImagine that we want to know whether police are racially biased in how they treat members of the public. One way to assess this is by using data from the police about the outcomes of their interactions with the public.\nFor example, we might want to know if people from minority ethnic backgrounds more likely to be arrested after a stop than people from white backgrounds (Knox et al)\nPolice collect data on stops - why not just run a regression on these data to see if people from minority ethnic backgrounds are more likely to be stopped?\nThe problem is we can’t just rely on data about police stops - “if police racially discriminate when choosing whom to investigate, analyses using administrative records to estimate racial discrimination in police behavior are statistically biased, and many quantities of interest are unidentified—even among investigated individuals—absent strong and untestable assumptions.”\nWe’re going to hear a lot about ‘strong and untestable assumptions’.\n“when there is any racial discrimination in the decision to detain civilians—a decision that determines which encounters appear in police administrative data at all—then estimates of the effect of civilian race on subsequent police behavior are biased absent additional data and/or strong and untestable assumptions.”\n[INSERT FIGURE 2 FROM KNOX]\n\n\n\nKnox et al (2020) FIGURE 2. Principal Strata and Observed Police–Civilian Encounters. Notes: The figure displays the four principal strata that comprise police–civilian encounters based on how the mediator M (whether a civilian is stopped by police) responds to treatment D (whether the civilian is a racial minority). Minorities in the “always stop” and anti-minority racial stop strata, highlighted in red, are stopped by police and, thus, appear in police administrative data. Likewise, white civilians in the “always-stop” and anti-white racial stop strata, highlighted in blue, appear in police data. “Never stop” encounters are unobserved. Because white and nonwhite encounters are drawn from different principal strata, the two groups are incomparable and estimates of causal quantities using observed encounters will be statistically biased absent additional assumptions.\n\n\nIf you only analyse data that are the result of police stops then your results will be biased. To analyse data on police stops to estimate racial bias, you also need to know the total number of encounters (for each ethnic group) – that is, including encounters that did not lead to a stop.\nOthers (Gaebler et al. 2022)suggest that maybe you can identify some aspects of discrimination in administrative data. This would be discrimination at some point in the process, not total discrimination. It’s really important to be clear about what it is you want to know – do you care about total discrimination, or discrimination in a particular part of the process (e.g. court sentencing and not policing?).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The data don't speak for themselves, part one: Measurement error, selection and confounding</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#footnotes",
    "href": "types-of-data.html#footnotes",
    "title": "2  Types of criminological data",
    "section": "",
    "text": "‘Generative modelling’ is - depressingly - a term that means different things to different audiences (Sankaran and Holmes 2023). Michael Betancourt talks about ‘narratively generative’ models (betancourtWhatProbabilisticStory?), which is a terminology I quite like - we can tell a story about how the data came to be and this can influence the model/s we fit.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#section",
    "href": "types-of-data.html#section",
    "title": "2  Types of criminological data",
    "section": "2.6 ",
    "text": "2.6 \n\n2.6.1 CVI data?\nhttps://www.scotland.police.uk/about-us/covid-19-police-scotland-response/enforcement-and-response-data/\n“In response to the introduction of The Health Protection (Coronavirus) (Restrictions) (Scotland) Regulations 2020 and Coronavirus Act 2020, Police Scotland developed a ‘Coronavirus Interventions’ (CVI) recording system. This system allowed Police Scotland to begin gathering data in relation to the public co-operation levels with the new legislation. This system relies on Police Officers manually updating the system with the co-operation level when they encounter an individual in contravention of the new legislation. Due to the manual input required to form this data set, the contents of this slide are indicative only. Actual figures will differ from those recorded on Crime Systems (please see further slide), may be subject to change, and cannot be considered Official Police Statistics. They do provide an indication of the public co-operation levels across Scotland.”\n“Notes on the CVI System The second data source was the Coronavirus Intervention (CVI) system, introduced by Police Scotland on 6 April 2020 in response to the introduction of the Coronavirus Act 2020 and associated Health Protection Regulations in Scotland.52 Data from the CVI were published throughout the pandemic on a weekly basis by Police Scotland until 17 November 2021. The main purpose of this system was to gather data on levels of public co-operation with the new Regulations, based on police officer interventions. The CVI system relies on manual updates from police officers about any interventions they have had with members of the public in respect of the legislation. It is not compulsory and, as a result, does not provide a comprehensive estimate of the total number of policing encounters. Nevertheless, it measures all policing-related activity (not just use of enforcement) and so provides a useful indicator of the relative use of different types of police activity in the context of the ‘Four Es strategy’, which was widely adopted by police forces across the UK in the context of the pandemic.53 To our knowledge, the CVI System is the only database of its kind to be used to measure the overall use of extended policing powers across the UK police forces from the start of the pandemic. Therefore, it provides an extremely useful source of complementary information to the FPN data. Comparison between FPN tickets and the CVI system Figure 22 shows a comparison of the number of tickets issued and the number recorded on the CVI as a seven-day rolling average between 27 March 2020 and 31 May 2021. The dark line shows the average number of FPNs issued, and the light line shows the average number recorded on Police Scotland’s CVI system. Both trend lines show four ‘phases’ of policing activity in relation to FPNs which are broadly reflective of the tightening and easing of restrictions in Scotland. The total number of FPN tickets actually issued (n = 20,410) was 20.9% higher than the number recorded on Police Scotland’s Coronavirus Intervention (CVI) system over the same period (n = 16,876)” https://www.law.ed.ac.uk/sites/default/files/2022-08/FPN%204th%20report%20-%20FINAL.pdf\n\n\n\n\nAplin, Rachael. 2019. “The Grey Figure of Crime: If It Isn’t Crimed, It Hasn’t Happened.” In Policing UK Honour-Based Abuse Crime, edited by Rachael Aplin, 101–52. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-18430-8_4.\n\n\nAtak, Kıvanç. 2020. “Beyond the Western Crime Drop: Violence, Property Offences, and the State in Turkey 1990–2016.” International Journal of Law, Crime and Justice 60 (March): 100373. https://doi.org/10.1016/j.ijlcj.2019.100373.\n\n\nBuil-Gil, David, Ian Brunton-Smith, Jose Pina-Sánchez, and Alexandru Cernat. 2022. “Comparing Measurements of Violent Crime in Local Communities: A Case Study in Islington, London.” Police Practice and Research 23 (4): 489–506. https://doi.org/10.1080/15614263.2022.2047047.\n\n\nCarr-Hill, Roy. 2013. “Missing Millions and Measuring Development Progress.” World Development 46 (June): 30–44. https://doi.org/10.1016/j.worlddev.2012.12.017.\n\n\nFohring, Stephanie. 2014. “Putting a Face on the Dark Figure: Describing Victims Who Don’t Report Crime.” Temida 17 (4): 3–18.\n\n\nHope, Tim. 2023. “The Effect of ‘Third Party’ Pressure on Police Crime Recording Practice.” CrimRxiv.\n\n\nKitsuse, John I., and Aaron V. Cicourel. 1963. “A Note on the Uses of Official Statistics.” Social Problems 11 (2): 131–39. https://doi.org/10.2307/799220.\n\n\nPina-Sánchez, Jose, Sara Geneletti, Ana Veiga, Ana Morales, and Eoin Guilfoyle. 2022. “Ethnic Disparities in Sentencing: Warranted or Unwarranted?” OSF. https://doi.org/10.31235/osf.io/k8bsg.\n\n\nSankaran, Kris, and Susan P. Holmes. 2023. “Generative Models: An Interdisciplinary Perspective.” Annual Review of Statistics and Its Application 10 (Volume 10, 2023): 325–52. https://doi.org/10.1146/annurev-statistics-033121-110134.\n\n\nScottish Women’s Aid. 2021. “Response to the Consultation on the Scottish Crime and Justice Survey (SCJS), December 2021.” https://womensaid.scot/wp-content/uploads/2022/04/SWA-response-to-Scottish-Crime-and-Justice-Survey-consultation.pdf.\n\n\nSimes, Jessica T., Brenden Beck, and John M. Eason. 2023. “Policing, Punishment, and Place: Spatial-Contextual Analyses of the Criminal Legal System.” Annual Review of Sociology 49 (1): 221–40. https://doi.org/10.1146/annurev-soc-031021-035328.\n\n\nVerlaan, Tim, and Samuel Langton. 2024. “On the Use of Inferential Statistics on Administrative Police Data.” In The Crime Data Handbook, 197–210. Bristol University Press.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#quantitative-bias-analysis",
    "href": "selection-measurement.html#quantitative-bias-analysis",
    "title": "4  Selection effects, bias and so on",
    "section": "4.4 Quantitative Bias Analysis",
    "text": "4.4 Quantitative Bias Analysis\nWhat Knox et al. suggest is part of a set of approaches called Probabilistic Bias Analysis. This includes things like understanding selection bias, unmeasured confounding and misclassification.\nThere are whole books written on this topic! So no general solutions here.\n\n4.4.1 Selection bias\n(Greenland 2014)\nWe only see people in our study if they meet some criterion (in other words, where S = 1), but we want to see everybody (i.e. people for whom S = 0)\n“But because we see only the relation of X to Y conditional on selection (S D 1), we must impute the unconditional relation of X to Y using probabilities of selection given what we do see (X and Y given S D 1).” In the Knox and colleagues example, this would require knowing the numbers of people who were observed by police but not stopped, in order to calculate the probabilities of selection into the stop dataset. However, we don’t know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset would know this.\nEven if we do know this for our particular dataset, there is no guarantee that selection into the data would hold in every case that we might want to generalize our results to. As such we’d need to consider how differences between the study populations may have affected response and selection (e.g. would selection probabilities from a US study map onto a study in Manchester? How about one in Glasgow?)\n\n\n4.4.2 Confounding\nConfounding describes a situation where there’s something that we know affects the outcome we’re interested in and/or our independent variables, but we don’t have a measure for this. In criminology, we might have measures of police stops but not offending (.e.g from self-reports). Offending is likely to be a big - but not the only driver - of whether a person has contact with the police.\n\nbecause we do not see U, we must impute its values using probabilities (bets) about the values of U given what we do see (again, X and Y ).\n\nThe tipr approach is to unmeasured confounding - not just that we have a variable measured inaccurately, but that there is a key variable we haven’t measured",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selection effects, bias and so on</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#example",
    "href": "selection-measurement.html#example",
    "title": "4  Selection effects, bias and so on",
    "section": "4.6 Example",
    "text": "4.6 Example\n\nwork in example of using rcme data… but applying a different type of bias analysis?\n\n\n\n\n\nBlackwell, Matthew, James Honaker, and Gary King. 2017. “A Unified Approach to Measurement Error and Missing Data: Overview and Applications.” Sociological Methods & Research 46 (3): 303–41. https://doi.org/10.1177/0049124115585360.\n\n\nGaebler, Johann, William Cai, Guillaume Basse, Ravi Shroff, Sharad Goel, and Jennifer Hill. 2022. “A Causal Framework for Observational Studies of Discrimination.” Statistics and Public Policy 9 (1): 26–48. https://doi.org/10.1080/2330443X.2021.2024778.\n\n\nGreenland, Sander. 2014. “Sensitivity Analysis and Bias Analysis.” In Handbook of Epidemiology, edited by Wolfgang Ahrens and Iris Pigeot, 685–706. New York, NY: Springer. https://doi.org/10.1007/978-0-387-09834-0_60.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selection effects, bias and so on</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#misclassification",
    "href": "selection-measurement.html#misclassification",
    "title": "4  Selection effects, bias and so on",
    "section": "4.4 Misclassification?",
    "text": "4.4 Misclassification?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selection effects, bias and so on</span>"
    ]
  },
  {
    "objectID": "simulation.html#limits-to-simulation-approaches",
    "href": "simulation.html#limits-to-simulation-approaches",
    "title": "4  The data don’t speak for themselves, part two: Presenting results",
    "section": "4.2 Limits to simulation approaches",
    "text": "4.2 Limits to simulation approaches\nBut remember that our model is wrong!! (Greenland) This approach incorporates the uncertainty as expressed by your model’s standard errors. This still makes a load of assumptions - basically that we’ve fit the right model and that we had the right data. In more complex cases - like most real-world research - we might be fitting a much more complicated statistical model with non-linear effects and interaction terms and all sorts. As we’ll discuss in the next section, we might also want to cast a critical eye over the data that we are analysing before drawing our conclusions.\nWe still need to bear in mind the generative story for our - data is our uncertainty due to survey sampling? What population does a victimization divide relate to?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The data don't speak for themselves, part two: Presenting results</span>"
    ]
  },
  {
    "objectID": "simulation.html#add-clarify-example-here",
    "href": "simulation.html#add-clarify-example-here",
    "title": "4  The data don’t speak for themselves, part two: Presenting results",
    "section": "4.1 Add clarify example here",
    "text": "4.1 Add clarify example here\nIn the example above we performed the simulation from each models’ variance-covariance matrix ourselves. Whilst it’s good to know how this works, in practice there are R packages which can do this for us. One good option is clarify.\nHere we reproduce our results using clarify. Instead of fitting a model to each year as above, clarify expects us to give it a single model object, so we re-express the two separate models (one for each year) as a single model, now including year as an interaction term with sex.\n\n# using clarify -----------------------------------------------------------\n\nlibrary(clarify)\n\n\nmod1 &lt;- glm(cbind(vict, n - vict) ~ fct_rev(sex) * year,\n                 family = \"binomial\",\n                 data = dat)\n\ns &lt;- sim(mod1,\n         n = n_sims)\n\n\nsim_fun2 &lt;- function(coefs) {\n  men_2015 &lt;- unname(coefs[\"fct_rev(sex)men\"])\n  men_interact &lt;- unname(coefs[\"fct_rev(sex)men:year2020\"])\n  \n  men_2020 &lt;- men_2015 + men_interact\n  \n  or_2015 &lt;- exp(men_2015)\n  or_2020 &lt;- exp(men_2020)\n  \n  victim_divide(or_2015, or_2020)\n  \n}\n\n\nest2 &lt;- sim_apply(s, \n                  FUN = sim_fun2)\n\ntibble(x = as.vector(est2)) |&gt; \n  reframe(vds = quantile(x, c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99)),\n          vals = c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99))\n\n# A tibble: 11 × 2\n       vds  vals\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.12   0.01 \n 2 -1.02   0.025\n 3 -0.948  0.05 \n 4 -0.860  0.1  \n 5 -0.710  0.25 \n 6 -0.524  0.5  \n 7 -0.296  0.75 \n 8 -0.0193 0.9  \n 9  0.210  0.95 \n10  0.469  0.975\n11  0.911  0.99 \n\n\nHere we can see that the results are basically the same.\nWhy clarify:\n\nget someone else to do the coding\nmore efficient\nexamples/tutorials online\nlink to marginal_effects?s\n\n\n4.1.0.1 A simple example: difference in probabilities?\nIn the example above we focused on calculating a new quantity of interest from the coefficients of a fitted regression model.\nThis drew on a specific (possibly quite niche) criminological example. This example is closely related to the general approach of calculating ‘marginal effects’. These techniques use the fact that with any general linear model we can get a predicted value of our outcome by plugging the coefficients into the regression equation to express effects of independent variables on the scale of the outcome. this could be a difference in the probability of being a victim of crime between men and women, or the difference in the estimated number of victimization incidents, or, well, anything you like really! Marginal effects are a very powerful technique by which we can understand model results and cover a variety of specific methods to calculate quantities we might be interested in form a fitted model (see https://marginaleffects.com/).\nThe fundamentals of all these methods are the same, but the details differ. I really recommend working through the guides at marginaleffects.com to get a sense of what is possible. Here we will work through a simple example. This is a very simple set-up to the clarify example we used, although with marginaleffects the focus is more on calculating predictions from the model (whereas in our example we converted coefficient values rather than predictions), and clarify focuses on simulation to express model uncertainty.\n[^8:] The ‘marginal’ terminology is very confusing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The data don't speak for themselves, part two: Presenting results</span>"
    ]
  },
  {
    "objectID": "ethics.html#community-loss",
    "href": "ethics.html#community-loss",
    "title": "6  Ethics",
    "section": "6.3 community loss",
    "text": "6.3 community loss\nJessica Simes (Jessica T. Simes 2021) gives a good example of how we may want to come up with theoretically informed measures, or theoretically re-frame measures. Simes analysed imprisonment data from the state of Massachusets in the USA, including spatial regression of prison admission rates and how these relate to “racial demographics, social and economic disadvantage, arrest rates, and violent crime” (Jessica T. Simes 2018).\nAs part of this analysis Simes suggests that the cumulative years sentenced to residents of a particular neighbourhood be thought of as ‘community loss’. This is not (just?) an indicator of individual punishment histories, but reflects the chronic and long-term exposure to loss due to imprisonment in different neighbourhoods. This highlights the effects of imprisonment on the communities in which people who end up in prison lived prior to their imprisonment, rather than just focusing on the people who are currently in prison.\n\n\n\n\nSimes, Jessica T. 2021. Punishing Places: The Geography of Mass Imprisonment. Univ of California Press.\n\n\nSimes, Jessica T. 2018. “Place and Punishment: The Spatial Context of Mass Incarceration.” Journal of Quantitative Criminology 34 (2): 513–33. https://doi.org/10.1007/s10940-017-9344-y.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#who-cares-about-data-provenance",
    "href": "types-of-data.html#who-cares-about-data-provenance",
    "title": "2  Types of criminological data",
    "section": "",
    "text": "“Why has the data been collected (and collected in this way)?\nHow has the data been collected and/or by whom or by what?\nWhat/who is included and what/who is excluded?\nWhat is the context for the data collection (routine activity, bespoke intervention, to meet a target)?\nHas the data been dis/aggregated or manipulated or cleaned in some other way to arrive at its present form?\nWhat are the relevant definitions and concepts that govern the data/data collection?”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#other-forms-of-crime-data",
    "href": "types-of-data.html#other-forms-of-crime-data",
    "title": "2  Types of criminological data",
    "section": "2.5 Other forms of crime data",
    "text": "2.5 Other forms of crime data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "types-of-data.html#triangulation-a-solution",
    "href": "types-of-data.html#triangulation-a-solution",
    "title": "2  Types of criminological data",
    "section": "2.6 Triangulation: a solution?",
    "text": "2.6 Triangulation: a solution?\nAtak - triangulation is good Buil-GIl says it’s best practice - … but need to assume that measures are of the same thing?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Types of criminological data</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#large-worlds-and-small-worlds",
    "href": "general-linear-model.html#large-worlds-and-small-worlds",
    "title": "3  The general linear model",
    "section": "",
    "text": "3.1.1 Big picture: two tribes of modellers?\n-   Economists versus epidemiologists",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#confounding",
    "href": "selection-measurement.html#confounding",
    "title": "5  The data don’t speak for themselves, part one: Measurement error, selection and confounding",
    "section": "5.4 Confounding",
    "text": "5.4 Confounding\nConfounding describes a situation where there’s something that we know affects the outcome we’re interested in and/or our independent variables, but we don’t have a measure for this. In criminology, we might have measures of police stops but not offending (.e.g from self-reports). Offending is likely to be a big - but not the only driver - of whether a person has contact with the police.\n\nbecause we do not see U, we must impute its values using probabilities (bets) about the values of U given what we do see (again, X and Y ).\n\nThe tipr approach is to unmeasured confounding - not just that we have a variable measured inaccurately, but that there is a key variable we haven’t measured\nneed some criminological examples here\nNeed to make assumptions about the magnitude of the bias to implement any of the technical fixes. Fine. But where does this information come from?\n“The preceding approach assumes that U is a known confounder (e.g., a smoking indicator) that was unmeasured in the study in question but has been previously identified and subject to study in relation to disease if not exposure. If instead U represents an unspecified, unknown confounder, then the entire sensitivity exercise will remain far more speculative. Nonetheless, decomposition of the bias factor can still be successful in demonstrating that only implausibly strong confounder or selection effects can account for a strong observed association. Cornfield et al. (1959) is considered a landmark study in which such an approach was used to examine claims that the smoking-lung cancer relation might be attributable to confounding”\n\nso this is based on the idea tat we can identify an “implausibly strong” confounder, which is reasonable.\nOne approach is to pick a bunch of possible bias parameters and test to see if results are robust to all of them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The data don't speak for themselves, part one: Measurement error, selection and confounding</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#should-we-all-be-doing-bias-analysis-all-the-time",
    "href": "selection-measurement.html#should-we-all-be-doing-bias-analysis-all-the-time",
    "title": "5  The data don’t speak for themselves, part one: Measurement error, selection and confounding",
    "section": "5.5 Should we all be doing bias analysis all the time?",
    "text": "5.5 Should we all be doing bias analysis all the time?\n“Despite these considerations, there is no basis for mandating a bias analysis of every study or even most studies. For example, bias analysis is superfluous when conventional intervals show that no useful conclusion could be drawn from the study even if it were perfect apart from random error. More generally, rather than providing a bias analysis, a study may provide greater service by refraining from inference; instead it can focus on carefully reporting its design, conduct, and data in great detail to facilitate pooling and meta-analysis (Greenland et al. 2004). Inferences are best based on a more complete account of evidence than can be provided in a single study report, and thus the effort of bias analysis is more justifiable in research synthesis (Turner et al. 2009; Welton et al. 2009). Even there, bias analysis becomes essential only when doing risk assessment or when authors claim to offer near-definitive conclusions.” (Greenland 2014, p703)\n… so you only need to bother with this stuff if you ‘claim to offer near-definitive conclusions’. Is your study likely to contribute to a meta analysis? Or in other words, when you are moving between the small world and the large world.\nSo the key thing is how we talk about our models - it us that moves between the small world and the large world.\nMeans humility when making policy recommendations!!\nIs the small world enough? How do we talk about this? e.g. if all we’ve done is analysed convictions data do we talk about offending, or just conviction?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The data don't speak for themselves, part one: Measurement error, selection and confounding</span>"
    ]
  },
  {
    "objectID": "simulation.html#whats-your-estimand",
    "href": "simulation.html#whats-your-estimand",
    "title": "4  The data don’t speak for themselves, part two: Presenting results",
    "section": "4.2 What’s your estimand?",
    "text": "4.2 What’s your estimand?\nLundberg, Johnson, and Stewart (2021) state that all analyst should be able to describe what their ‘estimand’ is. The estimand is the “target quantity” of your analysis. Lundberg and colleagues suggest an estimand as two compnents: a ‘unit specific quantity’ and a ‘target population’.\nThese target quantities can be observed outcomes (was a particular person a victim of crime), potential outcomes (would a person have been a victim of crime if they lived in a high-deprivation area?), or differences in potential outcomes (the effect of area deprivation on a person’s victim status). The target population describes over what units we are aggregating our ‘unit specific quantities’ - this may be the whole of Scotland, or a particular city, or a group of people with particular characteristics. Lunberg and colleagues describe a paper by “Harding and colleagues (2018) [who] estimate the effect of prison on labor market outcomes by leveraging random variation in judges’ propensities to sentence people convicted of felonies to probation instead of prison. The target population is offenders who would have been sentenced to probation rather than prison if they had faced a more lenient judge (Harding et al. 2018:67). This is a subpopulation that is conceptually interesting: individuals whose sentences might plausibly change if judges were encouraged to be more lenient in sentencing.”\nCrucially the link between the theoretical estimand and the empirical estimand that we actually try to learn from our dataset based on “assumptions about the relationship between the data we observe and the data we do not.”\n\n4.2.1 Quantities of interest\nSometimes when we fit a statistical model we just want to read off a coefficient in the model and that is our key result. However, sometimes we want to calculate some other quantity from our model and focus on that instead. For general applications of this approach see Gelman and Pardoe (2007) and King et al. (2000).\n\n4.2.1.1 A simple example: difference in probabilities?\n\n\n4.2.1.2 A complicated example: victimization divides\nFor example, (Hunter and Tseloni 2016) use the results of a fitted regression model to calculate a measure they call ‘Victimization Divide’. This measure is a way to describe how victimization inequality has changed over time - an important thing for criminologists to care about.\nThis measure is defined as\n(ratio of victimization rates in year 2 - 1) - (ratio of victimization rates in year 2 - 1) / (ratio of victimization rates in year one - 1)\nin R code this might look like\n\nvictim_divide &lt;- function(base_y1, base_y2){\n  ((base_y2 - 1) - (base_y1 - 1)) / (base_y1 - 1)\n}\n\nthis is analogous to exploring the percentage change in victimization inequality between two comparison years.\nTo calculate the ratio of victimization rates in years 1 and 2, Hunter and Tseloni fit a regression model (specifically a negative binomial model) and then use the coefficients from this model as inputs into the Victimization Divide formula. Based on this they conclude ADD SUMMARY OF THEIR FINDINGS\nHowever, Hunter and Tseloni’s data come from the Crime Survey for England and Wales. As such we should also be interested in the uncertainty in these coefficients. In that case it’s not quite so clear how to incorporate this uncertainty into the calculation of the VD. The process described by King et al. (2000) and implemented in the R package clarify (https://github.com/iqss/clarify) provides one solution.\nOne way to do this is to use the regression model to simulate a bunch of coefficients values and then calculate the VD for each one.\nMatthews and McVie (2024; see also Matthews 2024) demonstrate this process. The table below shows the prevalence of victimization (for all crime types) for men and women as reported in the Crime Survey for England and Wales for 2015 and 2020 (Office for National Statistics, n.d.). We can see that the prevalence of victimization was higher for men than women in both years, and that it increased for men (by 3.0 percentage points) and women (by 3.6 percentage points) between 2015 and 2020.\n\n# set seed and load packages\nset.seed(nchar(\"vict divide\") ^ 4)\n\nlibrary(MASS)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndat &lt;-\n  tribble(\n    ~prev, ~year, ~sex, ~n,\n    0.167, \"2015\", \"men\", 15030,\n    0.153, \"2015\", \"women\", 18320,\n    0.197, \"2020\", \"men\", 15505,\n    0.189, \"2020\", \"women\", 18230\n  )\n# calculate the number of victims\ndat &lt;-\n  dat |&gt; \n  mutate(vict = as.integer(n * prev))\n\nWe can fit a simple regression model to the data in each year to calculate the ratio of the odds of being a victim for men and women. Model 1 shows a statistically significant difference for men (compared to women) in 2015, with men having 11% greater odds of being a victim of crime. In contrast, Model 2 finds that men had a 5% greater odds of being a victim of crime than women in 2020; however, this difference does not meet the 95% threshold for statistical significance.\n\nmodel2020 &lt;- \n  glm(cbind(vict, n - vict) ~ fct_rev(sex),\nfamily = \"binomial\",\ndata = filter(dat, year == \"2020\"))\n\n\nmodel2015 &lt;- \n  glm(cbind(vict, n - vict) ~ fct_rev(sex),\nfamily = \"binomial\",\ndata = filter(dat, year == \"2015\"))\n\n\nresults_2020 &lt;-\nbroom::tidy(model2020) %&gt;%\nmutate(est = exp(estimate))\n\nresults_2015 &lt;-\nbroom::tidy(model2015) %&gt;%\nmutate(est = exp(estimate))\n\nCalculating the VD for these two results, based on the odds ratios from the two models, shows that victimization inequality decreased by 52% between the two years. Victimiztion inequality fell by more than half!\n\nmain_est &lt;- victim_divide(base_y1 = results_2015$est[[2]],\nbase_y2 = results_2020$est[[2]])\n\nHowever, this does not factor the uncertainty in these odds ratios into our results. To investigate this we can simulate 1,000 sets of coefficients from the models’ results. Using 10,000 simulations from the models’ results, we get 95% intervals for the VD of -0.95 and 0.22. Whilst our most of our simulations do indicate that victimization inequality has declined, this would not be considered ‘statistically significant’ at the usual 95% threshold.\n\nn_sims &lt;- 1e5\n\ndraws_2020 &lt;-\n  MASS::mvrnorm(\n    n_sims,\n    coef(model2020),\n    vcov(model2020)\n  )\n\ndraws_2015 &lt;-\n  MASS::mvrnorm(\n    n_sims,\n    coef(model2015),\n    vcov(model2015)\n  )\n\ndraws_2015 &lt;-\n  draws_2015 %&gt;%\n  as.data.frame() %&gt;%\n  as_tibble() %&gt;%\n  mutate(est = exp(`fct_rev(sex)men`))\n\ndraws_2020 &lt;-\n  draws_2020 %&gt;%\n  as.data.frame() %&gt;%\n  as_tibble() %&gt;%\n  mutate(est = exp(`fct_rev(sex)men`))\n\n# combine the results\nsim_dat &lt;-\n  tibble(\n    vd = victim_divide(base_y1 = draws_2015$est,\n                       base_y2 = draws_2020$est)\n  )\n\nsim_dat %&gt;%\n  reframe(vds = quantile(vd, c(0.025, 0.975),\n          vals = c(0.025, 0.975)))\n\n# A tibble: 2 × 1\n     vds\n   &lt;dbl&gt;\n1 -1.03 \n2  0.479\n\n\nThe beauty of this simulation approach described by King et al (2000) is that it generalizes to any DQI, such as VD, and to any regression model specification. Say that instead of the VD were interested in the absolute difference in predicted victimization after controlling for other factors (like a marginal effect). Or maybe we have fitted a count model and we want to know the number of people reporting 2 or more victimization incidents - we can calculate this from our simulations whilst incorporating (some) uncertainty into our estimates.\nMatthews and McVie (forthcoming) illustrate that there are many (many!) ways to empirically describe victimization inequality. Using these simulation methods means you can use a statistical model to describe any of these measures, and factor in the unceratinty implied by the model. This means that you have more scope to map your theoretical estimand onto your empirical results.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The data don't speak for themselves, part two: Presenting results</span>"
    ]
  },
  {
    "objectID": "ethics.html#reproducible-research-practices",
    "href": "ethics.html#reproducible-research-practices",
    "title": "6  Doing ethical quantitative criminology",
    "section": "6.2 Reproducible research practices?",
    "text": "6.2 Reproducible research practices?\nIt is an ethical responsibility of quantitative researchers to make sure that their analyses are (to the best of their capacity) correct and reproducible. This process is called ‘workflow’.\nThere have been high-profile cases where results were not correct, or turned out not to be reproducible - known as the ‘replication crisis’. Criminology has been late to the party for replication crises (Sweeten 2020), but in the last few years a few studies have identified potentially widespread Questionable Research Practices (Chin et al 2023)\nThis is an umbrella term used to describe how research designs can be changed during analysis in order to produce statistically significant results, and so increase the chances of a paper being published.\n\n6.2.1 Workflow good practices\nGayle 2022:\n(i) Use established data analysis tools (e.g. Stata, SPSS, R or SAS), and clearly state the version, and all the libraries, dependencies and plugins that are used. (ii) Clearly identify the version of the dataset and its origins (i.e. where and when it was obtained) using a persistent identifier such as a digital object identifier (DOI). (iii) Construct a data dictionary in a literate format that can easily be understood by someone unconnected with the project. (iv) Write down all of the code for how the data were prepared for analysis, in a literate format that can easily be understood by someone unconnected with the project. (v) Write down all of the code for all of the analyses undertaken, and not just the analyses that are presented in the published work, in a literate format that can easily be understood by someone unconnected with the project. (vi) Archive the project materials, bundled-up as a research object in a findable and accessible location, and endeavour to make them interoperable and re-useable in the future.\nThe last point may be difficult when working with administrative data (for example if this was accessed through a Data Sharing Agreement with a police force)\n\n\n6.2.2 An example: de-policing and crime\nThere have been some well-documented instances of poor workflow leading to erroneous conclusions (see discussion in Gayle 2022), but despite this established best practices are not widespread in criminology (Sweeten 2020).\nFor example, there was some controversy around a recent paper which suggested that ‘de-policing’ in Denver in 2020 led to increases in violent crime (Nix et al https://onlinelibrary.wiley.com/doi/full/10.1111/1745-9125.12363). However, another research who attempted to reproduce their findings - and fair play to Nix and colleagues for making their research reproducible - suggested that their main result was due to a merging error which led to the analytical dataset being scrambled (https://github.com/jkangbrown/when_police_replication).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Doing ethical quantitative criminology</span>"
    ]
  },
  {
    "objectID": "ethics.html#framing-the-tyranny-of-the-means",
    "href": "ethics.html#framing-the-tyranny-of-the-means",
    "title": "6  Doing ethical quantitative criminology",
    "section": "6.4 Framing: The tyranny of the means",
    "text": "6.4 Framing: The tyranny of the means\nWe should avoid essentializing.\nTypical focus of regression results is group average effect. Even if we have a statiistically significant difference between two groups in their estimated probability (or log-odds) of being convicted of comitting a crime, on its own this doesn’t tell us about how likely any individual member of those groups is to be convicted.\nBy focusing on how people with the same observables vary we can avoid essentializing people? (McCall 2005).\nThis is an added benefit of the simulation approach we discussed in the previous section - we can use these kind of methods to illustrate the variation of outcomes for people with the same observables, not just average differences in outcomes for people with different observables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Doing ethical quantitative criminology</span>"
    ]
  },
  {
    "objectID": "ethics.html#framing-the-importance-of-language",
    "href": "ethics.html#framing-the-importance-of-language",
    "title": "6  Doing ethical quantitative criminology",
    "section": "6.5 Framing: the importance of language",
    "text": "6.5 Framing: the importance of language\nMore conceptually, it’s important to think about how we frame the results of any analysis.\nThree visualizations from Data Feminism\n\nlanguage use\nproviding necessary context?\ndeficit narrative\n\n“…a narrative that reduces a social group to negative stereotypes and fails to portray them with creativity and agency.” (https://data-feminism.mitpress.mit.edu/pub/czq9dfs5/release/3)\nhttps://data-feminism.mitpress.mit.edu/pub/czq9dfs5#ndayi2fa1pk\n\nthe description of your charts is theoretically informed\n\nThe value of this exercise is not to say that any of these framings is ‘right’ (although for the reasons Klein and D’Ignazio outline we might find some preferable to others), but that they reflect different theoretical positions, and give different emphases to contextual factors - factors outside our datasets.\n“Placing numbers in context and naming racism or sexism when it is present in those numbers should be a requirement—not only for feminist data communication, but for data communication full stop.”\nData Feminism is mostly aimed towards data scientists and data journalists, not academics. Does this change how we should view their recommendations?\n(the original study: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4539829/)\n\n6.5.1 An example of theoretical framing: community loss\nJessica Simes (Jessica T. Simes 2021) gives a good example of how we may want to come up with theoretically informed measures, or theoretically re-frame measures. Simes analysed imprisonment data from the state of Massachusets in the USA, including spatial regression of prison admission rates and how these relate to “racial demographics, social and economic disadvantage, arrest rates, and violent crime” (Jessica T. Simes 2018).\nAs part of this analysis Simes suggests that the cumulative years sentenced to residents of a particular neighbourhood be thought of as ‘community loss’. This is not (just?) an indicator of individual punishment histories, but reflects the chronic and long-term exposure to loss due to imprisonment in different neighbourhoods. This highlights the effects of imprisonment on the communities in which people who end up in prison lived prior to their imprisonment, rather than just focusing on the people who are currently in prison.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Doing ethical quantitative criminology</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "7  Conclusion",
    "section": "",
    "text": "Summary:\n\nIn this workshop I have argued that the key to effective analysis of criminological data lies outside the spreadsheet.\nWe should start our analysis by understanding how our data came to be\nThis informs our choice of model\nAnd how we might choose to account for measurement error, selection effects or omitted variables (or not) in estimates from our models\nAnd in our choice of theoretical estimands, and how we use model results to come up with estimates of those estimands\nAnd in how we do our work (using workflow best practices) and how we talk about our results",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#footnotes",
    "href": "general-linear-model.html#footnotes",
    "title": "3  The general linear model",
    "section": "",
    "text": "okay, strictly speaking there are different varieties of negative binomial model. So… flavours? (Hilbe 2014)↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#why-not-just-use-a-linear-model",
    "href": "general-linear-model.html#why-not-just-use-a-linear-model",
    "title": "3  The general linear model",
    "section": "3.4 why not just use a linear model?",
    "text": "3.4 why not just use a linear model?\nIf you really want to you can just use a linear model for count data. This will still give you an accurate model of the mean outcome. However, there are two main problems with this approach.\nFirst, if you have small counts (say, if you were modelling homicides in Scotland) the uncertainty in the mean estimate may give you a confidence interval below zero:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nset.seed(12346)\n\nn_draws &lt;- 1e3\n\ndat &lt;- \ndata.frame(\n  y = rpois(n = n_draws,\n            lambda = 0.001) # draw from poisson distribution with mean 0.001\n)\n\ndat |&gt; \n  count(y)\n\n  y   n\n1 0 997\n2 1   3\n\nlm(y ~ 1, data = dat) |&gt; # fit intercept-only model with normal outcome\n  broom::tidy() |&gt; \n  mutate(conf_low = estimate - 1.96 * std.error)\n\n# A tibble: 1 × 6\n  term        estimate std.error statistic p.value  conf_low\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.003   0.00173      1.73  0.0833 -0.000391\n\nglm(y ~ 1, # fit intercept only model with poisson outcome\n    family = \"poisson\",\n    data = dat) |&gt; \n  broom::tidy() |&gt; \n  mutate(conf_low = estimate - 1.96 * std.error,\n         exp_est = exp(estimate),\n         exp_conf_low = exp(conf_low))\n\n# A tibble: 1 × 8\n  term       estimate std.error statistic  p.value conf_low exp_est exp_conf_low\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercep…    -5.81     0.577     -10.1 8.16e-24    -6.94 0.00300     0.000968\n\n\nHere the confidence intervals are not properly expressing what we know to be true about our data (that it has to be positive).\nSecond, the standard linear model assumes constant variance. But in practice we probably want more variance for larger counts.\nSee figure at https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html\n\n3.4.1 Interpreting coefficients from Poisson models\nPer UCLA OARC: “for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant.”\nThe most straightforward way to interpret coefficients from Poisson models is to exponentiate the coefficient value. This converts the beta coefficient into an Incident Rate Ratio (https://stats.oarc.ucla.edu/stata/output/poisson-regression/)\n“The exp(Intercept) is the baseline rate, and all other estimates would be relative to it.” (https://stats.stackexchange.com/a/11097)\nMostly though I would recommend using your model to come up with sensible estimates of counts of the thing you are interested in - we’ll come to this in Chapter Four.\n\n\n3.4.2 Specific problems with GLMs\n\n3.4.2.1 over-dispersion\n\npoisson assumes mean and variance are the same (in that there is only one term in the model, \\(\\lambda\\), which describes both the average and the variability around the average)\nThis is quite a brittle assumption though, and there’s no reason that real-life data has to obey it. So what? If your data are over-dispersed (the conditional variance exceeds the conditional mean), you will get standard errors that are too small.\nthis breaks basically all your inferences!\np-values are too optimistic\nExample of gun violence in Oakland: https://cao-94612.s3.amazonaws.com/documents/Oakland-Ceasefire-Evaluation-Final-Report-May-2019.pdf\n\n\n3.4.2.1.1 What do to? about over-dispersion\nIf all you care about is your standard errors you can use quasi-poisson (same point estimates as poisson but with ‘empirical’ SEs - similar to using robust standard errors). Francis et al. use this approach to modelling counts of victimization. In my experience it’s also faster than the standard alternative…\n… which is negative binomial binomial regression.\nThe negative binomial is a flavour of statistical model 1 for count data which has an extra “dispersion” parameter, which allows for over-dispersion. The poisson model can’t do this, because it has only one parameter (\\(\\lambda\\)) so the model has nowhere to ‘put’ any information about overdispersion.\nAt this point the details get a bit involved, but interested readers can see more in:\nHilbe (2014) for an accessible introduction and Hilbe (2011) for a deep-dive\nVer Hoef and Boveng (2007) give a blow-by-blow account of the differences between the two models in how they handle over-dispersion. But I would suggest that these distinctions depend on the kind of research question you are interested in. If all you care about is the mean for the two groups then you (might not care about over-dispersion at all)[https://www.statalist.org/forums/forum/general-stata-discussion/general/1570223-non-count-outcomes-and-use-of-poisson-regression?p=1570448#post1570448]. If you really care about modelling the extra dispersion in your negative binomial distribution (this isn’t very common but hey you might want to) then you can’t do this with a poisson model and you need something like negative binomial (see https://stats.stackexchange.com/a/568052)\n\n\n\n3.4.2.2 zero-inflation\ntake from Hilbe (distinct zeros? ZIP; generic solution; NB?)\n\nZIP can have different predictors for zeros than count part\nyou can also have different predictors for dispersion and rate parameter in NB if you want",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The general linear model</span>"
    ]
  },
  {
    "objectID": "simulation.html#the-tide-machine",
    "href": "simulation.html#the-tide-machine",
    "title": "4  The data don’t speak for themselves, part two: Presenting results",
    "section": "",
    "text": "4.1.1 Quantities of interest\n\n4.1.1.1 A simple example: difference in probabilities?\nWith any general linear model we can get a predicted value of our outcome by plugging the coefficients into the regression equation. Once we do this we can calculate quantities such as a difference in the probability of being a victim of crime between men and women, or the difference in the estimated number of victimization incidents.\nUsing model predictions to understand the results of models is a very general technique which can be applied to calculate various flavour of what is called ‘marginal effect’ [^8]. This is an umbrella term which covers a variety of specific methods to calculate quantities we might be interested in form a fitted model (see https://marginaleffects.com/).\nThe fundamentals of all these methods are the same, but the details differ. I really recommend working through the guides at marginaleffects.com to get a sense of what is possible. Here we will work through a simple example.\n[^8:] The ‘marginal’ terminology is very confusing.\n\n\n4.1.1.2 A complicated example: victimization divides\nFor example, (Hunter and Tseloni 2016) use the results of a fitted regression model to calculate a measure they call ‘Victimization Divide’. This measure is a way to describe how victimization inequality has changed over time - an important thing for criminologists to care about.\nThis measure is defined as\n(ratio of victimization rates in year 2 - 1) - (ratio of victimization rates in year 2 - 1) / (ratio of victimization rates in year one - 1)\nin R code this might look like\n\nvictim_divide &lt;- function(base_y1, base_y2){\n  ((base_y2 - 1) - (base_y1 - 1)) / (base_y1 - 1)\n}\n\nthis is analogous to exploring the percentage change in victimization inequality between two comparison years.\nTo calculate the ratio of victimization rates in years 1 and 2, Hunter and Tseloni fit a regression model (specifically a negative binomial model) and then use the coefficients from this model as inputs into the Victimization Divide formula. Based on this they conclude ADD SUMMARY OF THEIR FINDINGS\nHowever, Hunter and Tseloni’s data come from the Crime Survey for England and Wales. As such we should also be interested in the uncertainty in these coefficients. In that case it’s not quite so clear how to incorporate this uncertainty into the calculation of the VD. The process described by King et al. (2000) and implemented in the R package clarify (https://github.com/iqss/clarify) provides one solution.\nOne way to do this is to use the regression model to simulate a bunch of coefficients values and then calculate the VD for each one.\nMatthews and McVie (2024; see also Matthews 2024) demonstrate this process. The table below shows the prevalence of victimization (for all crime types) for men and women as reported in the Crime Survey for England and Wales for 2015 and 2020 (Office for National Statistics, n.d.). We can see that the prevalence of victimization was higher for men than women in both years, and that it increased for men (by 3.0 percentage points) and women (by 3.6 percentage points) between 2015 and 2020.\n\n# set seed and load packages\nset.seed(nchar(\"vict divide\") ^ 4)\n\nlibrary(MASS)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndat &lt;-\n  tribble(\n    ~prev, ~year, ~sex, ~n,\n    0.167, \"2015\", \"men\", 15030,\n    0.153, \"2015\", \"women\", 18320,\n    0.197, \"2020\", \"men\", 15505,\n    0.189, \"2020\", \"women\", 18230\n  )\n# calculate the number of victims\ndat &lt;-\n  dat |&gt; \n  mutate(vict = as.integer(n * prev))\n\nWe can fit a simple regression model to the data in each year to calculate the ratio of the odds of being a victim for men and women. Model 1 shows a statistically significant difference for men (compared to women) in 2015, with men having 11% greater odds of being a victim of crime. In contrast, Model 2 finds that men had a 5% greater odds of being a victim of crime than women in 2020; however, this difference does not meet the 95% threshold for statistical significance.\n\nmodel2020 &lt;- \n  glm(cbind(vict, n - vict) ~ fct_rev(sex),\nfamily = \"binomial\",\ndata = filter(dat, year == \"2020\"))\n\n\nmodel2015 &lt;- \n  glm(cbind(vict, n - vict) ~ fct_rev(sex),\nfamily = \"binomial\",\ndata = filter(dat, year == \"2015\"))\n\n\nresults_2020 &lt;-\nbroom::tidy(model2020) %&gt;%\nmutate(est = exp(estimate))\n\nresults_2015 &lt;-\nbroom::tidy(model2015) %&gt;%\nmutate(est = exp(estimate))\n\nCalculating the VD for these two results, based on the odds ratios from the two models, shows that victimization inequality decreased by 52% between the two years. Victimiztion inequality fell by more than half!\n\nmain_est &lt;- victim_divide(base_y1 = results_2015$est[[2]],\nbase_y2 = results_2020$est[[2]])\n\nHowever, this does not factor the uncertainty in these odds ratios into our results. To investigate this we can simulate 1,000 sets of coefficients from the models’ results. Using 10,000 simulations from the models’ results, we get 95% intervals for the VD of -0.95 and 0.22. Whilst our most of our simulations do indicate that victimization inequality has declined, this would not be considered ‘statistically significant’ at the usual 95% threshold.\n\nn_sims &lt;- 1e5\n\ndraws_2020 &lt;-\n  MASS::mvrnorm(\n    n_sims,\n    coef(model2020),\n    vcov(model2020)\n  )\n\ndraws_2015 &lt;-\n  MASS::mvrnorm(\n    n_sims,\n    coef(model2015),\n    vcov(model2015)\n  )\n\ndraws_2015 &lt;-\n  draws_2015 %&gt;%\n  as.data.frame() %&gt;%\n  as_tibble() %&gt;%\n  mutate(est = exp(`fct_rev(sex)men`))\n\ndraws_2020 &lt;-\n  draws_2020 %&gt;%\n  as.data.frame() %&gt;%\n  as_tibble() %&gt;%\n  mutate(est = exp(`fct_rev(sex)men`))\n\n# combine the results\nsim_dat &lt;-\n  tibble(\n    vd = victim_divide(base_y1 = draws_2015$est,\n                       base_y2 = draws_2020$est)\n  )\n\nsim_dat %&gt;%\n  reframe(vds = quantile(vd, c(0.025, 0.975),\n          vals = c(0.025, 0.975)))\n\n# A tibble: 2 × 1\n     vds\n   &lt;dbl&gt;\n1 -1.03 \n2  0.479\n\n\nThe beauty of this simulation approach described by King et al (2000) is that it generalizes to any DQI, such as VD, and to any regression model specification. Say that instead of the VD were interested in the absolute difference in predicted victimization after controlling for other factors (like a marginal effect). Or maybe we have fitted a count model and we want to know the number of people reporting 2 or more victimization incidents - we can calculate this from our simulations whilst incorporating (some) uncertainty into our estimates.\nMatthews and McVie (forthcoming) illustrate that there are many (many!) ways to empirically describe victimization inequality. Using these simulation methods means you can use a statistical model to describe any of these measures, and factor in the unceratinty implied by the model. This means that you have more scope to map your theoretical estimand onto your empirical results.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The data don't speak for themselves, part two: Presenting results</span>"
    ]
  },
  {
    "objectID": "selection-measurement.html#residual-or-unmeasured-confounding",
    "href": "selection-measurement.html#residual-or-unmeasured-confounding",
    "title": "5  The data don’t speak for themselves, part one: Measurement error, selection and confounding",
    "section": "5.4 Residual or unmeasured confounding",
    "text": "5.4 Residual or unmeasured confounding\nConfounding describes a situation where there’s something that we know affects the outcome we’re interested in and/or our independent variables, but we don’t have a measure for this. In criminology, we might have measures of police stops but not offending (.e.g from self-reports). Offending is likely to be a big - but not the only driver - of whether a person has contact with the police.\n\nbecause we do not see U, we must impute its values using probabilities (bets) about the values of U given what we do see (again, X and Y ).\n\nThe tipr approach is to unmeasured confounding - not just that we have a variable measured inaccurately, but that there is a key variable we haven’t measured, although Peto suggests that these in practice are hard to distinguish\nneed some criminological examples here\nNeed to make assumptions about the magnitude of the bias to implement any of the technical fixes. Fine. But where does this information come from?\n“The preceding approach assumes that U is a known confounder (e.g., a smoking indicator) that was unmeasured in the study in question but has been previously identified and subject to study in relation to disease if not exposure. If instead U represents an unspecified, unknown confounder, then the entire sensitivity exercise will remain far more speculative. Nonetheless, decomposition of the bias factor can still be successful in demonstrating that only implausibly strong confounder or selection effects can account for a strong observed association. Cornfield et al. (1959) is considered a landmark study in which such an approach was used to examine claims that the smoking-lung cancer relation might be attributable to confounding”\n\nso this is based on the idea tat we can identify an “implausibly strong” confounder, which is reasonable.\nOne approach is to pick a bunch of possible bias parameters and test to see if results are robust to all of them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The data don't speak for themselves, part one: Measurement error, selection and confounding</span>"
    ]
  },
  {
    "objectID": "ethics.html#practical",
    "href": "ethics.html#practical",
    "title": "6  Doing ethical quantitative criminology",
    "section": "6.3 Practical",
    "text": "6.3 Practical",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Doing ethical quantitative criminology</span>"
    ]
  },
  {
    "objectID": "simulation.html#using-the-clarify-package",
    "href": "simulation.html#using-the-clarify-package",
    "title": "4  The data don’t speak for themselves, part two: Presenting results",
    "section": "4.1 Using the clarify package",
    "text": "4.1 Using the clarify package\nIn the example above we performed the simulation from each models’ variance-covariance matrix ourselves. Whilst it’s good to know how this works, in practice there are R packages which can do this for us. One good option is clarify.\nHere we reproduce our results using clarify. Instead of fitting a model to each year as above, clarify expects us to give it a single model object, so we re-express the two separate models (one for each year) as a single model, now including year as an interaction term with sex. The code below is a bit involved, but\n\n# using clarify -----------------------------------------------------------\n\nlibrary(clarify)\n\n\nmod1 &lt;- glm(cbind(vict, n - vict) ~ fct_rev(sex) * year,\n                 family = \"binomial\",\n                 data = dat)\n\ns &lt;- sim(mod1,\n         n = n_sims)\n\n\nsim_victim_divide &lt;- function(coefs) {\n  men_2015 &lt;- unname(coefs[\"fct_rev(sex)men\"])\n  men_interact &lt;- unname(coefs[\"fct_rev(sex)men:year2020\"])\n  \n  men_2020 &lt;- men_2015 + men_interact\n  \n  or_2015 &lt;- exp(men_2015)\n  or_2020 &lt;- exp(men_2020)\n  \n  victim_divide(or_2015, or_2020)\n  \n}\n\n\nest2 &lt;- sim_apply(s, \n                  FUN = sim_victim_divide)\n\ntibble(x = as.vector(est2)) |&gt; \n  reframe(vds = quantile(x, c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99)),\n          vals = c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99))\n\n# A tibble: 11 × 2\n       vds  vals\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.12   0.01 \n 2 -1.02   0.025\n 3 -0.948  0.05 \n 4 -0.860  0.1  \n 5 -0.710  0.25 \n 6 -0.524  0.5  \n 7 -0.296  0.75 \n 8 -0.0193 0.9  \n 9  0.210  0.95 \n10  0.469  0.975\n11  0.911  0.99 \n\n\nHere we can see that the results are basically the same as our own simulation. Importantly there is inherent uncertainty in simulation results, so it’s crucial to set the seed for the random number generator so you get the same results every time. You also need to run enough simulation draws to get a good estimate of the uncertainty. The default for clarify is 1000 draws. More simulations are always better, but will take longer - so there is a pragmatic aspect to how much time and computer resource you have to run simulations.\nWe have conducted an initial simulation ourselves to understand how the process works, but in general I would recommend using a package like clarify if you conduct this kind of simulation. It’s likely that professionally developed and maintained software will be less error-prone and more efficient than writing your own!\n\n4.1.0.1 A simple example: difference in probabilities?\nIn the example above we focused on calculating a new quantity of interest from the coefficients of a fitted regression model.\nThis drew on a specific (possibly quite niche) criminological example. This example is closely related to the general approach of calculating ‘marginal effects’. These techniques use the fact that with any general linear model we can get a predicted value of our outcome by plugging the coefficients into the regression equation to express effects of independent variables on the scale of the outcome. this could be a difference in the probability of being a victim of crime between men and women, or the difference in the estimated number of victimization incidents, or, well, anything you like really! Marginal effects are a very powerful technique by which we can understand model results and cover a variety of specific methods to calculate quantities we might be interested in form a fitted model (see https://marginaleffects.com/).\nThe fundamentals of all these methods are the same, but the details differ. I really recommend working through the guides at marginaleffects.com to get a sense of what is possible. Here we will work through a simple example. This is a very simple set-up to the clarify example we used, although with marginaleffects the focus is more on calculating predictions from the model (whereas in our example we converted coefficient values rather than predictions), and clarify focuses on simulation to express model uncertainty.\n[^8:] The ‘marginal’ terminology is very confusing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The data don't speak for themselves, part two: Presenting results</span>"
    ]
  }
]