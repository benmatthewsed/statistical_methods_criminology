# The general linear model




## Large worlds and small worlds

Richard McElreath [@mcelreathStatisticalRethinkingBayesian2020] talks about the 'small world' of the model and the 'large world' that we actually live in. Our spreadsheets and coefficients can only summarize the small world for us, and omit some of the complexity of the large world. This is fine if all we want to do is provide summaries of the the numbers in our spreadsheets. But as soon as we want to understand the large world we can run in to problems if all we focus on are the rows and columns in front of us. We often need to bring our understanding of the large world - for example, how an incident becomes a crime - to bear during statistical modelling. In this session we'll overview two ways in which our understanding of the 'large world' might influence how we interpret our results from the 'small world': measurement error and selection effects.

### Big picture: two tribes of modellers?

    -   Economists versus epidemiologists



## Intro to GLM

Things we don't cover:
- Won't include: mixed/multilevel/hierarchical/etc
- Additive models (GAMs)
- 'bespoke' models



Recommended watching: https://www.youtube.com/watch?v=qbxNf4iqJPo&t=1215s

Introduction to LM

-   Stochastic (or random) component
-   Systematic component
-   Link function

(I adapted this notation from Solomon Kurz https://bookdown.org/content/4857/god-spiked-the-integers.html#poisson-regression)

$$
\begin{align*}
y_i \sim & {Distribution} (\theta_i, \phi) \\
{f(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

where $\theta_i$ is the parameter of interest (e.g., the probability of 1 in a Binomial distribution) and $\phi$ is a placeholder for any other parameters necessary for the likelihood but not typically of primary substantive interest (e.g., $\sigma$ in conventional Gaussian models). The $f(\cdot)$ portion is the link function.

For the linear model, we have:

$$
\begin{align*}
y_i \sim & {Normal} (\theta_i, \sigma) \\
{Identity(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

> an example here?

One key feature of a GLM is that you can get predictions for your outcome by adding up the coefficients for each element of your systematic component, and then applying the appropriate transformation through the link function.


### Logistic regression

For logistic regression we have:

$$
\begin{align*}
y_i \sim & {Binomial} (n, p_i) \\
{logit(p_i)} & = \alpha + \beta (x_i),
\end{align*}
$$ for logistic regression, $n$ = 1, and we are just interested in modelling $p_i$.

These kinds of models are very common in the social sciences, including in criminology. We might want to model whether someone is a victim of crime or not, or whether the person has been convicted of a crime in a given year, or whether they are confident in the police or not... and so on and so on. The nice thing about this model formulation is that the ${logit}$ link function makes sure that all the probabilities the model estimates are between 0 and 1. Strictly speaking we don't need to do this (see the linear probability model), but it's nice to avoid results that obviously don't make sense, like negative probabilities and such.




## Count data

Count data are counts of things. That's it! This means that they non-negative whole numbers. Count data are common in criminology when it comes to modelling crime - e.g. the number of crimes reported to the police, or the number of victimization incidents experienced by victims. 

Whilst we may see crime data be re-expressed as *rates* per 1,000 population, before this they are counts.




The foundational model for count data is the Poisson model:

$$
\begin{align*}
y_i \sim & {Poisson} (\lambda) \\
{log(\lambda)} & = \alpha + \beta (x_i),
\end{align*}
$$

Now there is only one parameter (lambda; $\lambda$) that we are modelling, unlike with linear regression. This means that in Poisson models the mean and the variance are assumed to be the same (or put another way, that they are both direct functions of $\lambda$).

The coefficients from Poisson models (which range from $-\infty$ to $\infty$) are converted to be predicted counts that are non-negative integers via the $\log$ link function. 

__Graph of log here?__



## why not just lm?

- You can get negative predictions
- LM assumes constant variance - we probably want more variance for larger counts

### Specific problems with GLMs

#### over-dispersion

-   poisson assumes mean and variance are the same (only one term in the model)

-   so what? mostly under-estimates the standard error

-   p-values are too optimistic

-   you need to think about why your data are over-dispersed

-   if all you care about is your standard errors you can use quasi-poisson (same point estimates as poisson but with 'empirical' SEs - similar to using robust standard errors)

- Francis et al. use this approach to modelling counts of victimization

- You can also use negative binomial


#### zero-inflation

take from Hilbe (distinct zeros? ZIP; generic solution; NB?)

-   ZIP can have different predictors for zeros than count part

-   you can also have different predictors for dispersion and rate parameter in NB if you want





## Assumptions and p-values

https://bristoluniversitypressdigital.com/edcollchap/book/9781529232073/ch014.xml

Focus on effect size measures instead?

Think about populations hard (https://benmatthewsed.github.io/what_to_do_odds_ratios/what_to_do_odds_ratios.html#/title-slide)

### An example of thinking about populations and generalization

This question came from the Policing the Pandemic in Scotland project with excellent colleagues Dr Vicky Gorton and Prof Susan McVie

We linked data on all (well, most) fines received for breaching the Covid regulations in Scotland between 27 March 2020 to 31 May 2021 with information on recipients’ health (service use) and (some) social circumstances (I’m not going to go into detail about this)

We also have the same information on a comparison group of matched controls (matched by age, sex, Local Authority and SIMD decile)

We want to know if people with more ‘vulnerability’ (read - health service use) are more likely than others to have received a Covid fine (FPN)

Having done all this, I actually don’t think we’ll use this in our paper. Thinking hard about the population that we’re interested in made me wonder…

… and what’s wrong with an odds ratio of 35 anyway?

This is an accurate description of our dataset!

If the problem is that we don’t think a result this extreme would generalize to another ‘sample’ from the sample population - with close to every person who received an FPN do we even have any issues of generalizability (we have basically 100% of the relevant people, minus matching error)?

Instead of generalizability, I think we have either a massive issue with transportability/external validity (Degtiar and Rose 2023), or we have no issue at all

It seems nonsensical to suggest that these results would apply to another country during Covid or another pandemic (countries were very different in their responses)

The results for Lockdown One in Scotland don’t even generalize to Lockdown Two - we show that in our analysis!
