# The general linear model




## Large worlds and small worlds

Richard McElreath [@mcelreathStatisticalRethinkingBayesian2020] talks about the 'small world' of the model and the 'large world' that we actually live in. Our spreadsheets and coefficients can only summarize the small world for us, and omit some of the complexity of the large world - all models are wrong, but some are useful and all that. [To some extent, this is good!](https://kwarc.info/teaching/TDM/Borges.pdf) 

And all we want to do is provide summaries of the the numbers in our spreadsheets we have no problem. But as soon as we want to understand the large world we can run in to problems if all we focus on are the rows and columns in front of us. 

Later on we'll consider how we can bring our understanding of the large world - for example, how an incident becomes a crime - to bear during statistical modelling. In this session we'll overview the most common[^3] way in which criminologists understand the 'small world' of their spreadsheets: the general linear model.

[^3:] Probably?

## Intro to GLM

If you are taking this course I assume that you have some familiarity with linear models of some description. It is standard in quantitative methods training for social scientists to discuss the linear model, so I'll only provide a brief refresher here before moving on to discuss models that are more typical in criminology.

I should say that general linear models are complicated things, and it would be possible to spend months and months studying nothing but general linear models and their various extensions. Some specific flavours of model that I don't cover, but which may be useful for your own work include:
- Won't include: mixed/multilevel/hierarchical/etc (see e.g. https://www.cmm.bris.ac.uk/lemma/)
- Additive models (GAMs) (which are less common in criminology, but are very useful; https://noamross.github.io/gams-in-r-course/)
- 'bespoke' models (very infrequently used in criminology to my knowledge, https://betanalpha.github.io/assets/case_studies/generative_modeling.html, but see e.g. https://josepinasanchez.uk/wp-content/uploads/2018/09/bsc-presentation.pdf)



Recommended watching: https://www.youtube.com/watch?v=qbxNf4iqJPo&t=1215s

Introduction to LM

-   Stochastic (or random) component
-   Systematic component
-   Link function that converts between the parameter estimates and the form of the outcome (we'll say more about this later)

(I adapted this notation from Solomon Kurz https://bookdown.org/content/4857/god-spiked-the-integers.html#poisson-regression)

$$
\begin{align*}
y_i \sim & {Distribution} (\theta_i, \phi) \\
{f(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

where $\theta_i$ is the parameter of interest (e.g., the probability of 1 in a Binomial distribution) and $\phi$ is a placeholder for any other parameters necessary for the likelihood but not typically of primary substantive interest (e.g., $\sigma$ in conventional Gaussian models). The $f(\cdot)$ portion is the link function.

For the linear model, we have:

$$
\begin{align*}
y_i \sim & {Normal} (\theta_i, \sigma) \\
{Identity(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

> an example here?

One key feature of a GLM is that you can get predictions for your outcome by adding up the coefficients for each element of your systematic component, and then applying the appropriate transformation through the link function.


### Logistic regression

For logistic regression we have:

$$
\begin{align*}
y_i \sim & {Binomial} (n, p_i) \\
{logit(p_i)} & = \alpha + \beta (x_i),
\end{align*}
$$ for logistic regression, $n$ = 1, and we are just interested in modelling $p_i$.

These kinds of models are very common in the social sciences, including in criminology. We might want to model whether someone is a victim of crime or not, or whether the person has been convicted of a crime in a given year, or whether they are confident in the police or not... and so on and so on. The nice thing about this model formulation is that the ${logit}$ link function makes sure that all the probabilities the model estimates are between 0 and 1. Strictly speaking we don't need to do this (see the linear probability model), but it's nice to avoid results that obviously don't make sense, like negative probabilities and such.




## Count data

Count data are counts of things. That's it! This means that they non-negative whole numbers. Count data are common in criminology when it comes to modelling crime - e.g. the number of crimes reported to the police, or the number of victimization incidents experienced by victims. 

Whilst we may see crime data be re-expressed as *rates* per 1,000 population, before this they are counts.




The foundational model for count data is the Poisson model:

$$
\begin{align*}
y_i \sim & {Poisson} (\lambda) \\
{log(\lambda)} & = \alpha + \beta (x_i),
\end{align*}
$$

Now there is only one parameter (lambda; $\lambda$) that we are modelling, unlike with linear regression. This means that in Poisson models the mean and the variance are assumed to be the same (or put another way, that they are both direct functions of $\lambda$).

The coefficients from Poisson models (which range from $-\infty$ to $\infty$) are converted to be predicted counts that are non-negative integers via the $\log$ link function. This lets us have coefficients which can take any value (-2! 0.5!), but then convert these to counts as our count outcome demands.

__Graph of log here?__





## why not just use a linear model?

If you really want to you can just use a linear model for count data. This will still give you an accurate model of the mean outcome. However, there are two main problems with this approach.

First, if you have small counts (say, if you were modelling homicides in Scotland) the uncertainty in the mean estimate may give you a confidence interval below zero:

```{r}
library(dplyr)

set.seed(12346)

n_draws <- 1e3

dat <- 
data.frame(
  y = rpois(n = n_draws,
            lambda = 0.001) # draw from poisson distribution with mean 0.001
)

dat |> 
  count(y)

lm(y ~ 1, data = dat) |> # fit intercept-only model with normal outcome
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error)


glm(y ~ 1, # fit intercept only model with poisson outcome
    family = "poisson",
    data = dat) |> 
  broom::tidy() |> 
  mutate(conf_low = estimate - 1.96 * std.error,
         exp_est = exp(estimate),
         exp_conf_low = exp(conf_low))

```

Here the confidence intervals are not properly expressing what we know to be true about our data (that it has to be positive). 


Second, the standard linear model assumes constant variance. But in practice we probably want more variance for larger counts. 

See figure at https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html


### Interpreting coefficients from Poisson models

Per UCLA OARC: "for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant."

The most straightforward way to interpret coefficients from Poisson models is to exponentiate the coefficient value. This converts the beta coefficient into an Incident Rate Ratio (https://stats.oarc.ucla.edu/stata/output/poisson-regression/)

"The exp(Intercept) is the baseline rate, and all other estimates would be relative to it." (https://stats.stackexchange.com/a/11097)

Mostly though I would recommend using your model to come up with sensible estimates of counts of the thing you are interested in - we'll come to this in Chapter Four.


### Specific problems with GLMs

#### over-dispersion

-   poisson assumes mean and variance are the same (in that there is only one term in the model, $\lambda$, which describes both the average and the variability around the average)

- This is quite a brittle assumption though, and there's no reason that real-life data has to obey it. So what? If your data are over-dispersed (the conditional variance exceeds the conditional mean), you will get standard errors that are too small.

- this breaks basically all your inferences!

-   p-values are too optimistic


- Example of gun violence in Oakland: https://cao-94612.s3.amazonaws.com/documents/Oakland-Ceasefire-Evaluation-Final-Report-May-2019.pdf

##### What do to? about over-dispersion

If all you care about is your standard errors you can use quasi-poisson (same point estimates as poisson but with 'empirical' SEs - similar to using robust standard errors). Francis et al. use this approach to modelling counts of victimization. In my experience it's also faster than the standard alternative...

... which is negative binomial binomial regression. 

The negative binomial is a flavour of statistical model ^[okay, strictly speaking there are different varieties of negative binomial model. So... flavours? [@hilbeModelingCountData2014]] for count data which has an extra "dispersion" parameter, which allows for over-dispersion. The poisson model can't do this, because it has only one parameter ($\lambda$) so the model has nowhere to 'put' any information about overdispersion.

At this point the details get a bit involved, but interested readers can see more in: 

@hilbeModelingCountData2014 for an accessible introduction
and @hilbeNegativeBinomialRegression2011 for a deep-dive

@verhoefQuasiPoissonVsNegative2007 give a blow-by-blow account of the differences between the two models in how they handle over-dispersion. But I would suggest that these distinctions depend on the kind of research question you are interested in. If all you care about is the mean for the two groups then you (might not care about over-dispersion at all)[https://www.statalist.org/forums/forum/general-stata-discussion/general/1570223-non-count-outcomes-and-use-of-poisson-regression?p=1570448#post1570448]. If you really care about modelling the extra dispersion in your negative binomial distribution (this isn't very common but hey you might want to) then you can't do this with a poisson model and you need something like negative binomial (see https://stats.stackexchange.com/a/568052)






#### zero-inflation

take from Hilbe (distinct zeros? ZIP; generic solution; NB?)

-   ZIP can have different predictors for zeros than count part

-   you can also have different predictors for dispersion and rate parameter in NB if you want

# Practical

- In this practical we'll fit some count models in R

