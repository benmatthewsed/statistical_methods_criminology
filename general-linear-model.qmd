# The general linear model

## Large worlds and small worlds

Richard McElreath [@mcelreathStatisticalRethinkingBayesian2020] talks about the 'small world' of the model and the 'large world' that we actually live in. Our spreadsheets and coefficients can only summarize the small world for us, and omit some of the complexity of the large world. This is fine if all we want to do is provide summaries of the the numbers in our spreadsheets. But as soon as we want to understand the large world we can run in to problems if all we focus on are the rows and columns in front of us. We often need to bring our understanding of the large world - for example, how an incident becomes a crime - to bear during statistical modelling. In this session we'll overview two ways in which our understanding of the 'large world' might influence how we interpret our results from the 'small world': measurement error and selection effects.


## Intro to GLM


Recommended watching: https://www.youtube.com/watch?v=qbxNf4iqJPo&t=1215s

Introduction to LM

-   Stochastic (or random) component
-   Systematic component
-   Link function

(I adapted this notation from Solomon Kurz https://bookdown.org/content/4857/god-spiked-the-integers.html#poisson-regression)

$$
\begin{align*}
y_i \sim & {Distribution} (\theta_i, \phi) \\
{f(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

where $\theta_i$ is the parameter of interest (e.g., the probability of 1 in a Binomial distribution) and $\phi$ is a placeholder for any other parameters necessary for the likelihood but not typically of primary substantive interest (e.g., $\sigma$ in conventional Gaussian models). The $f(\cdot)$ portion is the link function.

For the linear model, we have:

$$
\begin{align*}
y_i \sim & {Normal} (\theta_i, \sigma) \\
{Identity(\theta_i)} & = \alpha + \beta (x_i - \bar x),
\end{align*}
$$

> an example here?

## Logistic regression

For logistic regression we have:

$$
\begin{align*}
y_i \sim & {Binomial} (n, p_i) \\
{logit(p_i)} & = \alpha + \beta (x_i),
\end{align*}
$$ for logistic regression, $n$ = 1, and we are just interested in modelling $p_i$.

These kinds of models are very common in the social sciences, including in criminology. We might want to model whether someone is a victim of crime or not, or whether the person has been convicted of a crime in a given year, or whether they are confident in the police or not... and so on and so on. The nice thing about this model formulation is that the ${logit}$ link function makes sure that all the probabilities the model estimates are between 0 and 1. Strictly speaking we don't need to do this (see the linear probability model), but it's nice to avoid results that obviously don't make sense, like negative probabilities and such.

## Count data

The foundational model for count data is the Poisson model:

$$
\begin{align*}
y_i \sim & {Poisson} (\lambda) \\
{log(\lambda)} & = \alpha + \beta (x_i),
\end{align*}
$$

Now there is only one parameter (lambda; $\lambda$) that we are modelling, unlike with linear regression. This means that in Poisson models the mean and the variance are assumed to be the same (or put another way, that they are both direct functions of $\lambda$).

The key feature of count data is that it is integers (i.e. whole numbers) only and they are stricly positive - you can't get negative counts. Pretty much all aggregated administrative data about crime in its raw form will be count data. This might be counts of recorded crime, counts of people released from prison and so on. These data might be re-expressed as *rates* per 1,000 population, but before this they are counts.

## why not just lm?

-   two tribes

    -   if all you care about is the average difference then you can just use 'Simple Mean Difference'
    -   you might be an economist
    -   there are lots of other interesting quesitons we might care about (e.g. modelling dispersion?)

### Specific problems with GLMs

-   classic problem - over-dispersion

-   poisson assumes mean and variance are the same (only one term in the model)

-   so what? mostly under-estimates the standard error

-   p-values are too optimistic

-   you need to think about why your data are over-dispersed

-   take from Hilbe (distinct zeros? ZIP; generic solution; NB?)

-   ZIP can have different predictors for zeros than count part

-   you can also have different predictors for dispersion and rate parameter in NB if you want

-   if all you care about is your standard errors you can use quasi-poisson (same point estimates as poisson but with 'empirical' SEs - similar to using robust standard errors)

## A note on bespoke models

I am extremely partial to bespoke models. If you have the time I would recommend reading and watching all the Statistical Rethinking lectures - this gives an introduction into building your own models tailored to your specific data and problems. But that will take weeks and weeks and we are only here until five.

## Assumptions and p-values

https://bristoluniversitypressdigital.com/edcollchap/book/9781529232073/ch014.xml

Focus on effect size measures instead?

Think about populations hard (https://benmatthewsed.github.io/what_to_do_odds_ratios/what_to_do_odds_ratios.html#/title-slide)

### An example of thinking about populations and generalization

This question came from the Policing the Pandemic in Scotland project with excellent colleagues Dr Vicky Gorton and Prof Susan McVie

We linked data on all (well, most) fines received for breaching the Covid regulations in Scotland between 27 March 2020 to 31 May 2021 with information on recipients’ health (service use) and (some) social circumstances (I’m not going to go into detail about this)

We also have the same information on a comparison group of matched controls (matched by age, sex, Local Authority and SIMD decile)

We want to know if people with more ‘vulnerability’ (read - health service use) are more likely than others to have received a Covid fine (FPN)

Having done all this, I actually don’t think we’ll use this in our paper. Thinking hard about the population that we’re interested in made me wonder…

… and what’s wrong with an odds ratio of 35 anyway?

This is an accurate description of our dataset!

If the problem is that we don’t think a result this extreme would generalize to another ‘sample’ from the sample population - with close to every person who received an FPN do we even have any issues of generalizability (we have basically 100% of the relevant people, minus matching error)?

Instead of generalizability, I think we have either a massive issue with transportability/external validity (Degtiar and Rose 2023), or we have no issue at all

It seems nonsensical to suggest that these results would apply to another country during Covid or another pandemic (countries were very different in their responses)

The results for Lockdown One in Scotland don’t even generalize to Lockdown Two - we show that in our analysis!
