# general linear model

# Intro to GLM

Recommended watching: https://www.youtube.com/watch?v=qbxNf4iqJPo&t=1215s

Introduction to LM

$$
\begin{equation}
y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\end{equation}
$$

where:

$$
\begin{align*}
y & : \text{Dependent variable (response)} \\
x_i & : \text{Independent variable (predictor)} \quad \text{for } i = 1, 2, \ldots, p \\
\alpha & : \text{Intercept term} \\
\beta_i & : \text{Coefficient for the } i\text{-th predictor} \quad \text{for } i = 1, 2, \ldots, p \\
\epsilon & : \text{Error term}
\end{align*}
$$

# Binary data

-   victim or not? offender or not? confident or not?

-   \[take from gary king\]

# Count data

-   integers only! stricly positive!

-   why not just lm?

-   two tribes

    -   if all you care about is the average difference then you can just use 'Simple Mean Difference'
    -   you might be an economist
    -   there are lots of other interesting quesitons we might care about (e.g. modelling dispersion?)

-   classic problem - over-dispersion

-   poisson assumes mean and variance are the same (only one term in the model)

-   so what? mostly under-estimates the standard error

-   p-values are too optimistic

-   you need to think about why your data are over-dispersed

-   take from Hilbe (distinct zeros? ZIP; generic solution; NB?)

-   ZIP can have different predictors for zeros than count part

-   you can also have different predictors for dispersion and rate parameter in NB if you want

-   if all you care about is your standard errors you can use quasi-poisson (same point estimates as poisson but with 'empirical' SEs - similar to using robust standard errors)

## A note on bespoke models


I am extremely partial to bespoke models. If you have the time I would recommend reading and watching all the Statistical Rethinking lectures - this gives an introduction into building your own models tailored to your specific data and problems. But that will take weeks and weeks and we are only here until five.


## Assumptions and p-values

https://bristoluniversitypressdigital.com/edcollchap/book/9781529232073/ch014.xml

Focus on effect size measures instead?

Think about populations hard (https://benmatthewsed.github.io/what_to_do_odds_ratios/what_to_do_odds_ratios.html#/title-slide)

### An example of thinking about populations and generalization

This question came from the Policing the Pandemic in Scotland project with excellent colleagues Dr Vicky Gorton and Prof Susan McVie

We linked data on all (well, most) fines received for breaching the Covid regulations in Scotland between 27 March 2020 to 31 May 2021 with information on recipients’ health (service use) and (some) social circumstances (I’m not going to go into detail about this)

We also have the same information on a comparison group of matched controls (matched by age, sex, Local Authority and SIMD decile)

We want to know if people with more ‘vulnerability’ (read - health service use) are more likely than others to have received a Covid fine (FPN)

Having done all this, I actually don’t think we’ll use this in our paper. Thinking hard about the population that we’re interested in made me wonder…

… and what’s wrong with an odds ratio of 35 anyway?

This is an accurate description of our dataset!

If the problem is that we don’t think a result this extreme would generalize to another ‘sample’ from the sample population - with close to every person who received an FPN do we even have any issues of generalizability (we have basically 100% of the relevant people, minus matching error)?

Instead of generalizability, I think we have either a massive issue with transportability/external validity (Degtiar and Rose 2023), or we have no issue at all

It seems nonsensical to suggest that these results would apply to another country during Covid or another pandemic (countries were very different in their responses)

The results for Lockdown One in Scotland don’t even generalize to Lockdown Two - we show that in our analysis!

