# Selection effects, bias and so on

We may be skeptial of conclusions drawn from 'small world' of our data and model for lots of reasons. In this section we'll briefly introduce three ways in which we may want to interrogate our statistical models. Books and books have been written about each topic, so this will only offer a brief overview of each, with links to further reading for those who want to know more.

The common thread running through all these measures is that they rely on being skeptical about the numbers in our spreadsheets. Either these numbers may be inaccurate (measurement error); they may omit some variables that we theoretically think are important (confounding), or; they omit some people/cases we care about (selection effects).

The data never speak for themselves.


## Measurement error

The standard linear model assumes that all variables are measured perfectly (or in other words, with no error).

In practice we know this is unlikely, particularly with crime data. For example, we know that police recorded crime data is not a perfect measure of the amount of crime 'out there' in society. As we have discussed already, not all crimes are reported to the police, not all incidents which are reported are recorded as crimes and so on.

Measurement error can impact an analysis in pretty much any conceiveable way, depending on the type, magnitude and source of measurement error - whether the error is in the outcome, or in a key independent variable, or in a control variable. It may bias your regression coefficients, meaning that your results are valid and in reality there is a stronger association between predictor and outcome that you have observed. It may not bias your regression coefficients at all but just impact the precision of your results. Other than in simple scenarios *we may not know what impact it is having*.

__More examples here__


### What to do about it?

What's more is that it's not entirely clear what to do about it.

One way to approach this, if we know or can reasonably approximate the model for the measurement error then we can use Bayesian methods to jointly estimate a model for our observations and the measurement error. This is easier said than done! Bayesian methods are extremely flexible and allow the researcher to model to specify a model for the measurement error which is estimated jointly with the model for their outcome. This relies on knowing what the right model for the measurement error should be. It also relies on fitting a Bayesian model to your data which comes with its own set of challenges.

Pina-Sanchez has written about measurement error in crime.

One approach is to use Bayesian methods. 

Pina Sanchez gave a workshop on Bayesian adjustments for measurement error, and you can find the materials here: [https://josepinasanchez.uk/short-courses/](https://josepinasanchez.uk/short-courses/)

https://josepinasanchez.uk/wp-content/uploads/2021/12/rmef_measurement-error_b.pdf

from https://josepinasanchez.uk/short-courses/

### SIMEX

A less involved option involves assessment measurement error through simulation. The process here is to fit the model to the data as observed and try to figure out what the coefficients would be if there was no measurement error in the data. The `simex` R package lets you simulate new versions of your dataset with *more* and more measurement error. By looking to see how your coefficients change after re-fitting your model with increasing measurement error you can project backwards to what the coefficients would be with zero measurement error. Neat!

However, SIMEX works best when there is only one variable with measurement error in the analysis, and can be more difficult to compute when there are multiple variables with measurement error [@blackwellUnifiedApproachMeasurement2017]


### `rcme`

Pina-Sanchez and colleagues have written an R package that can conduct sensitivity analysis for some types of measurement error common to working with police recorded crime.

> Work through their example here? https://osf.io/preprints/socarxiv/sbc8w

Open questions - what about survey data? Models other than linear models?



## Selection effects

One use of regression models is to identify 'causal' effects of predictors on outcomes.


[@greenlandSensitivityAnalysisBias2014]

We only see people in our study if they meet some criterion (in other words, where S = 1), but we want to see everybody (i.e. people for whom S = 0)

"But because we see only the relation of X to Y conditional on selection (S D 1), we must impute the unconditional relation of X to Y using probabilities of selection given what we do see (X and Y given S D 1)." In the Knox and colleagues example, this would require knowing the numbers of people who were observed by police but not stopped, in order to calculate the probabilities of selection into the stop dataset. However, we don't know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset _would_ know this.

Even if we _do_ know this for our particular dataset, there is no guarantee that selection into the data would hold in every case that we might want to generalize our results to. As such we'd need to consider how differences between the study populations may have affected response and selection (e.g. would selection probabilities from a US study map onto a study in Manchester? How about one in Glasgow?)



_Some links on where to read more about causal inference? Maybe [here](https://www.r-causal.org/) _

Imagine that we want to know whether police are racially biased in how they treat members of the public. One way to assess this is by using data from the police about the outcomes of their interactions with the public.

For example, we might want to know if people from minority ethnic backgrounds more likely to be arrested after a stop than people from white backgrounds (Knox et al)

Police collect data on stops - why not just run a regression on these data to see if people from minority ethnic backgrounds are more likely to be stopped?

The problem is we can’t just rely on data about police stops - “if police racially discriminate when choosing whom to investigate, analyses using administrative records to estimate racial discrimination in police behavior are statistically biased, and many quantities of interest are unidentified—even among investigated individuals—absent strong and untestable assumptions.”

We're going to hear a lot about 'strong and untestable assumptions'.

“when there is any racial discrimination in the decision to detain civilians—a decision that determines which encounters appear in police administrative data at all—then estimates of the effect of civilian race on subsequent police behavior are biased absent additional data and/or strong and untestable assumptions.”

\[INSERT FIGURE 2 FROM KNOX\]

![Knox et al (2020) FIGURE 2. Principal Strata and Observed Police–Civilian Encounters. Notes: The figure displays the four principal strata that comprise police–civilian encounters based on how the mediator M (whether a civilian is stopped by police) responds to treatment D (whether the civilian is a racial minority). Minorities in the “always stop” and anti-minority racial stop strata, highlighted in red, are stopped by police and, thus, appear in police administrative data. Likewise, white civilians in the “always-stop” and anti-white racial stop strata, highlighted in blue, appear in police data. “Never stop” encounters are unobserved. Because white and nonwhite encounters are drawn from different principal strata, the two groups are incomparable and estimates of causal quantities using observed encounters will be statistically biased absent additional assumptions.](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20200730201026963-0363:S0003055420000039:S0003055420000039_fig2.png)

If you only analyse data that are the result of police stops then your results will be biased. To analyse data on police stops to estimate racial bias, you also need to know the total number of encounters (for each ethnic group) – that is, including encounters that did not lead to a stop.

Others [@gaeblerCausalFrameworkObservational2022]suggest that maybe you can identify some aspects of discrimination in administrative data. This would be discrimination at some point in the process, not total discrimination. It’s really important to be clear about what it is you want to know – do you care about total discrimination, or discrimination in a particular part of the process (e.g. court sentencing and not policing?).



## Solutions?

Knox et al. (2020) suggest some technical fixes, but emphasise that - if we are interested in using statistical models to identify _causal_ relationships _there is no general solution_ that can guarantee that coefficients in a regression model will have valid causal interpretations based on administrative data derived from police records. The key thing is thinking through the process by which the dataset was constructed, and conveying this to your reader.

__maybe get the students to play about with the data in the dataverse?https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KFQOCV__



### Confounding

Confounding describes a situation where there's something that we know affects the outcome we're interested in and/or our independent variables, but we don't have a measure for this. In criminology, we might have measures of police stops but not *offending* (.e.g from self-reports). Offending is likely to be a big - but not the only driver - of whether a person has contact with the police.

-   because we do not see U, we must impute its values using probabilities (bets) about the values of U given what we do see (again, X and Y ).



_The tipr approach is to unmeasured confounding - not just that we have a variable measured inaccurately, but that there is a key variable we haven't measured_


_need some criminological examples here_




## In general

Need to make assumptions about the magnitude of the bias to implement any of the technical fixes. Fine. But where does this information come from?

"The preceding approach assumes that U is a known confounder (e.g., a smoking indicator) that was unmeasured in the study in question but has been previously identified and subject to study in relation to disease if not exposure. If instead U represents an unspecified, unknown confounder, then the entire sensitivity exercise will remain far more speculative. Nonetheless, decomposition of the bias factor can still be successful in demonstrating that only implausibly strong confounder or selection effects can account for a strong observed association. Cornfield et al. (1959) is considered a landmark study in which such an approach was used to examine claims that the smoking-lung cancer relation might be attributable to confounding"

-   so this is based on the idea tat we can identify an "implausibly strong" confounder, which is reasonable.

- One approach is to pick a bunch of possible bias parameters and test to see if results are robust to all of them.



"Despite these considerations, there is no basis for mandating a bias analysis of every study or even most studies. For example, bias analysis is superfluous when conventional intervals show that no useful conclusion could be drawn from the study even if it were perfect apart from random error. More generally, rather than providing a bias analysis, a study may provide greater service by refraining from inference; instead it can focus on carefully reporting its design, conduct, and data in great detail to facilitate pooling and meta-analysis (Greenland et al. 2004). Inferences are best based on a more complete account of evidence than can be provided in a single study report, and thus the effort of bias analysis is more justifiable in research synthesis (Turner et al. 2009; Welton et al. 2009). Even there, bias analysis becomes essential only when doing risk assessment or when authors claim to offer near-definitive conclusions." [@greenlandSensitivityAnalysisBias2014 p703]

... so you only need to bother with this stuff if you 'claim to offer near-definitive conclusions'. Is your study likely to contribute to a meta analysis? Or in other words, when you are moving between the small world and the large world.

Means humility when making policy recommendations!!

_Is the small world enough? How do we talk about this? e.g. if all we've done is analysed convictions data do we talk about offending, or just conviction?_

