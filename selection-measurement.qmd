# The data don't speak for themselves, part one: Measurement error, selection and confounding

The data never speak for themselves.

In the previous chapter we introduced the idea of the 'small world' of the model. There are lots of reasons we may be skeptical about applying conclusions drawn from 'small world' of our data and model to the 'large world' we live in. In this section we'll briefly introduce three ways in which we may want to interrogate our statistical models. Books and books have been written about each topic, so this will only offer a brief overview of each, with links to further reading for those who want to know more.

The common thread running through all these measures is that they rely on being skeptical about the numbers in our spreadsheets. Either these numbers may be inaccurate (measurement error); they may omit some variables that we theoretically think are important (confounding), or; they omit some people/cases we care about (selection effects). All of these topics are quite specialized, so I only provide an overview here.

## Measurement error

Measurement error is the gap between what we are conceptually interested in and the way that this concept is recorded in our spreadsheet.

Measurement error can be present in the outcome variable, a focal independent variable or in a control variable, each of which can have different implications for analysis. The standard linear model which we have just been discussing assumes that all variables are measured perfectly (or in other words, with no error). If it is in the outcome variable it may not bias your regression coefficients at all but just impact the precision of your results (meaning that they are less likely to be statistically significant). Measurement error in your key independent variable may bias your regression coefficients *downwards*, meaning that your results are valid and in reality there is a stronger association between predictor and outcome that you have observed. If there is more noise in a control than in a key independent variable, measurement error in the control variable may lead to 'under controlling' - and finding statistically significant coefficients where there are none. Other than in simple scenarios *we may not know what impact it is having*.

Add discussion and simulations from https://gist.github.com/benmatthewsed/4aade247c6645698090c1b3949c2b3c7?


In practice we know criminological data are likely to be measured with some degree of error. For example, we know that police recorded crime data is not a perfect measure of the amount of crime 'out there' in society. As we have discussed already, not all crimes are reported to the police, not all incidents which are reported are recorded as crimes and so on.


Overview video: see https://www.youtube.com/watch?v=B0ZpenVs8oI

### Example One: Measurement error in police recorded crime data

-   From our discussion in Session One, we know that crime data recorded by the police are not a complete record of all crimes experienced in society in a given period - only crimes which are reported and recorded make it into recorded crime data.
-   So if we are interested in, for example, how many crimes there were in Scotland in 2023, the number of crimes recorded by police is likely to be an *under* count.
-   The many years of work comparing crimes recorded to the police with victimization surveys - the 'dark figure of crime' - attests to this.

### Example Two: Measurement error in victimizaton survey data

-   Historically, national victimization surveys (such as the SCJS and CSEW) capped the number of victim forms that victims could complete. In 2019, ONS said that "Since the survey began in 1981, “repeat” incidents have been limited to a total of 5. Historically, including a maximum of 5 repeat incidents for any individual victim had proven to be an effective way of reducing the effects of sample variability from year to year. This approach enabled the publication of incident rates that were not subject to large fluctuation between survey years. This approach yields a more reliable picture of changes in victimisations over time once high order repeat victimisations were treated in this way." (https://doc.ukdataservice.ac.uk/doc/7280/mrdoc/pdf/7280_csew_improving_victimisation_estimates_2019.pdf)

-   However, it also means that people who experienced more than five incidents of a particular 'series' crime type did not have their data accurately recorded

-   This was particularly important for women's reporting of violent victimization (Walby et al. 2015) - women who experienced domestic violence may well report more than five repeat incidents of victimization in a given year. A second measurement error issue came from the '97 code' - the option to report the number of incidents experienced as '96/too many to count'. Instead, based on the domestic violence literature Walby and colleagues propose using an estimate of 60 incidents for those who report the 97 code.

-   Sometimes it's possible to use uncapped data

### What to do about it?

It depends on the context.

One way to approach this, if we know or can reasonably approximate the model for the measurement error then we can use Bayesian methods to jointly estimate a model for our observations and the measurement error. This is easier said than done! Bayesian methods are extremely flexible and allow the researcher to model to specify a model for the measurement error which is estimated jointly with the model for their outcome. This relies on knowing what the right model for the measurement error should be. It also relies on fitting a Bayesian model to your data which comes with its own set of challenges.

Pina-Sanchez has written about measurement error in crime.

Pina Sanchez gave a workshop on Bayesian adjustments for measurement error, and you can find the materials here: <https://josepinasanchez.uk/short-courses/>

https://josepinasanchez.uk/wp-content/uploads/2021/12/rmef_measurement-error_b.pdf

from https://josepinasanchez.uk/short-courses/

### SIMEX

A less involved option involves assessment measurement error through simulation. The process here is to fit the model to the data as observed and try to figure out what the coefficients would be if there was no measurement error in the data. The `simex` R package lets you simulate new versions of your dataset with *more* and more measurement error. By looking to see how your coefficients change after re-fitting your model with increasing measurement error you can project backwards to what the coefficients would be with zero measurement error. Neat!

However, SIMEX works best when there is only one variable with measurement error in the analysis, and can be more difficult to compute when there are multiple variables with measurement error [@blackwellUnifiedApproachMeasurement2017]

### `rcme`

Pina-Sanchez and colleagues have written an R package that can conduct sensitivity analysis for some types of measurement error common to working with police recorded crime.

> Work through their example here? https://osf.io/preprints/socarxiv/sbc8w

Open questions - what about survey data? Models other than linear models? What about measurement error in independent variables?

## Selection effects

Selection bias arises when there are people who we would have liked to observe in our study but we don't observe them, and this lack of observation is related to their characteristics. Put another way, we can think of selection bias as affecting the rows of our dataset - there are some rows that are missing that we would like to see.

[@greenlandSensitivityAnalysisBias2014]

We only see people in our study if they meet some criterion (in other words, where S = 1), but we want to see everybody (i.e. also people for whom S = 0)

"But because we see only the relation of X to Y conditional on selection (S D 1), we must impute the unconditional relation of X to Y using probabilities of selection given what we do see (X and Y given S D 1)."

Bushway (2007) gives an example from sentencing research: say we want to investigate racial discrimination in sentencing. A common approach has been to focus on whether people from different ethnic backgrounds receive different lengths of prison sentence, after statistically adjusting for other factors such as the offence type, age at time of offence and so on [@pina-sanchezTacklingSelectionBias2020]. However this creates a selection bias problem - not all convictions lead to custodial sentences, and so focusing only on custodial sentences means that the analyst is focusing on discrimination "*conditional* on being selected into the incarcerated population" [@bushwayMagicStillThere2007a]. Any results derived would then not relate to all convicted people.

Add more from (https://www-jstor-org.ezproxy-s2.stir.ac.uk/stable/2095230?seq=8)?

In the Knox and colleagues example, this would require knowing the numbers of people who were observed by police but not stopped, in order to calculate the probabilities of selection into the stop dataset. However, we don't know this - and it is hard to imagine a scenario where an analyst of an police administrative dataset *would* know this.

Even if we *do* know this for our particular dataset, there is no guarantee that selection into the data would hold in every case that we might want to generalize our results to. As such we'd need to consider how differences between the study populations may have affected response and selection (e.g. would selection probabilities from a US study map onto a study in Manchester? How about one in Glasgow?)

*Some links on where to read more about causal inference? Maybe [here](https://www.r-causal.org/)*

Imagine that we want to know whether police are racially biased in how they treat members of the public. One way to assess this is by using data from the police about the outcomes of their interactions with the public.

For example, we might want to know if people from minority ethnic backgrounds more likely to be arrested after a stop than people from white backgrounds (Knox et al)

Police collect data on stops - why not just run a regression on these data to see if people from minority ethnic backgrounds are more likely to be stopped?

The problem is we can’t just rely on data about police stops - “if police racially discriminate when choosing whom to investigate, analyses using administrative records to estimate racial discrimination in police behavior are statistically biased, and many quantities of interest are unidentified—even among investigated individuals—absent strong and untestable assumptions.”

We're going to hear a lot about 'strong and untestable assumptions'.

“when there is any racial discrimination in the decision to detain civilians—a decision that determines which encounters appear in police administrative data at all—then estimates of the effect of civilian race on subsequent police behavior are biased absent additional data and/or strong and untestable assumptions.”

\[INSERT FIGURE 2 FROM KNOX\]

![Knox et al (2020) FIGURE 2. Principal Strata and Observed Police–Civilian Encounters. Notes: The figure displays the four principal strata that comprise police–civilian encounters based on how the mediator M (whether a civilian is stopped by police) responds to treatment D (whether the civilian is a racial minority). Minorities in the “always stop” and anti-minority racial stop strata, highlighted in red, are stopped by police and, thus, appear in police administrative data. Likewise, white civilians in the “always-stop” and anti-white racial stop strata, highlighted in blue, appear in police data. “Never stop” encounters are unobserved. Because white and nonwhite encounters are drawn from different principal strata, the two groups are incomparable and estimates of causal quantities using observed encounters will be statistically biased absent additional assumptions.](https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20200730201026963-0363:S0003055420000039:S0003055420000039_fig2.png)

If you only analyse data that are the result of police stops then your results will be biased. To analyse data on police stops to estimate racial bias, you also need to know the total number of encounters (for each ethnic group) – that is, including encounters that did not lead to a stop.

Others [@gaeblerCausalFrameworkObservational2022]suggest that maybe you can identify some aspects of discrimination in administrative data. This would be discrimination at some point in the process, not total discrimination. It’s really important to be clear about what it is you want to know – do you care about total discrimination, or discrimination in a particular part of the process (e.g. court sentencing and not policing?).

## Solutions?

Knox et al. (2020) suggest some technical fixes, but emphasise that - if we are interested in using statistical models to identify *causal* relationships *there is no general solution* that can guarantee that coefficients in a regression model will have valid causal interpretations based on administrative data derived from police records. The key thing is thinking through the process by which the dataset was constructed, and conveying this to your reader.

**maybe get the students to play about with the data in the dataverse?https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KFQOCV**

## Residual or unmeasured confounding

Confounding describes a situation where there's something that we know affects the outcome we're interested in and/or our independent variables, but we don't have a measure for this. In criminology, we might have measures of police stops but not *offending* (.e.g from self-reports). Offending is likely to be a big - but not the only driver - of whether a person has contact with the police.

-   because we do not see U, we must impute its values using probabilities (bets) about the values of U given what we do see (again, X and Y ).

*The tipr approach is to unmeasured confounding - not just that we have a variable measured inaccurately, but that there is a key variable we haven't measured*, although Peto suggests that these in practice are hard to distinguish

*need some criminological examples here*

Need to make assumptions about the magnitude of the bias to implement any of the technical fixes. Fine. But where does this information come from?

"The preceding approach assumes that U is a known confounder (e.g., a smoking indicator) that was unmeasured in the study in question but has been previously identified and subject to study in relation to disease if not exposure. If instead U represents an unspecified, unknown confounder, then the entire sensitivity exercise will remain far more speculative. Nonetheless, decomposition of the bias factor can still be successful in demonstrating that only implausibly strong confounder or selection effects can account for a strong observed association. Cornfield et al. (1959) is considered a landmark study in which such an approach was used to examine claims that the smoking-lung cancer relation might be attributable to confounding"

-   so this is based on the idea tat we can identify an "implausibly strong" confounder, which is reasonable.

-   One approach is to pick a bunch of possible bias parameters and test to see if results are robust to all of them.

## Should we all be doing bias analysis all the time?

"Despite these considerations, there is no basis for mandating a bias analysis of every study or even most studies. For example, bias analysis is superfluous when conventional intervals show that no useful conclusion could be drawn from the study even if it were perfect apart from random error. More generally, rather than providing a bias analysis, a study may provide greater service by refraining from inference; instead it can focus on carefully reporting its design, conduct, and data in great detail to facilitate pooling and meta-analysis (Greenland et al. 2004). Inferences are best based on a more complete account of evidence than can be provided in a single study report, and thus the effort of bias analysis is more justifiable in research synthesis (Turner et al. 2009; Welton et al. 2009). Even there, bias analysis becomes essential only when doing risk assessment or when authors claim to offer near-definitive conclusions." [@greenlandSensitivityAnalysisBias2014 p703]

... so you only need to bother with this stuff if you 'claim to offer near-definitive conclusions'. Is your study likely to contribute to a meta analysis? Or in other words, when you are moving between the small world and the large world.

So the key thing is how we talk about our models - it *us* that moves between the small world and the large world.

Means humility when making policy recommendations!!

*Is the small world enough? How do we talk about this? e.g. if all we've done is analysed convictions data do we talk about offending, or just conviction?*

# Practical

-   We're going to revisit the generative stories that you came up with in Section One. Is there possible:
    -   Measurement error?
    -   Selection effects?
    -   Omitted variables?
-   Write a vignette describing some results and then critique?
